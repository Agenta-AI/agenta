---
title: "Configuring Evaluators"
sidebar_label: "Configuring Evaluators"
description: "Learn how to configure evaluators for human evaluation"
sidebar_position: 2
---

import Image from "@theme/IdealImage";

## Creating evaluators

If you don't have evaluators yet, click **Create new** in the **Evaluator** section.

Each evaluator has:
- **Name** - What you're measuring (e.g., "correctness")
- **Description** - What the evaluator does
- **Feedback types** - How evaluators will score responses

For example, a "correctness" evaluator might have:
- `is_correct` - A yes/no question about accuracy
- `error_type` - A multiple-choice field for categorizing mistakes

<Image
  style={{
    display: "block",
    textAlign: "center",
    marginBottom: "20px",
  }}
  img={require("/images/evaluation/human-evaluation/configuring-evaluators.png")}
  alt="Creating evaluators for human evaluation"
  loading="lazy"
/>

## Available feedback types

- **Boolean** - Yes/no questions
- **Integer** - Whole number ratings
- **Decimal** - Precise numerical scores
- **Single-choice** - Pick one option
- **Multi-choice** - Pick multiple options
- **String** - Free-text comments or notes

## Grouping related feedback types

:::tip
Evaluators can include multiple related feedback types. For example:

**Correctness evaluator:**
- `is_correct` - Yes/no question about accuracy
- `error_type` - Multiple-choice field to categorize mistakes (only if incorrect)

**Style adherence evaluator:**
- `is_adherent` - Yes/no question about style compliance
- `comment` - Text field explaining why the style doesn't match (if needed)

This grouping helps you evaluate different aspects of your LLM's performance in an organized way.
:::

## Selecting evaluators

After creating evaluators:

1. Select the evaluators you want to use
2. You can use multiple evaluators in a single evaluation
3. Each evaluator will appear in the annotation interface

## Next steps

- Learn about [running evaluations](/evaluation/human-evaluation/running-evaluations)
- Understand [how to view results](/evaluation/human-evaluation/viewing-results)
- Try [A/B testing](/evaluation/human-evaluation/ab-testing)
