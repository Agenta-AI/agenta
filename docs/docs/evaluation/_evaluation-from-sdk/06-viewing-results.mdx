---
title: "Viewing Results"
sidebar_label: "Viewing Results"
description: "Learn how to retrieve and analyze evaluation results using the SDK"
sidebar_position: 6
---

<!-- TODO: Replace with new SDK evaluation content -->

## Fetching overall results

As soon as the evaluation is done, we can fetch the overall results:

```python
response = client.evaluations.fetch_evaluation_results('667d98fbd1812781f7e3761a')

results = [
    (evaluator["evaluator_config"]["name"], evaluator["result"])
    for evaluator in response["results"]
]
print(results)
```

## Fetching detailed results

Get detailed results for each test case:

```python
detailed_results = client.evaluations.fetch_evaluation_scenarios(
    evaluations_ids='667d98fbd1812781f7e3761a'
)
print(detailed_results)
```

## Analyzing results

<!-- TODO: Add content for analyzing results -->

## Exporting results

<!-- TODO: Add content for exporting results -->

## Next steps

- Learn about [human evaluation](/evaluation/human-evaluation/quick-start)
- Explore [comparing evaluations in the UI](/evaluation/evaluation-from-ui/comparing-runs)
- See all [evaluator types](/evaluation/configure-evaluators/overview)
