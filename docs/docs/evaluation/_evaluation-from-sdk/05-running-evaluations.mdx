---
title: "Running Evaluations"
sidebar_label: "Running Evaluations"
description: "Learn how to run evaluations programmatically using the SDK"
sidebar_position: 5
---

<!-- TODO: Replace with new SDK evaluation content -->

## Running an evaluation

First, let's grab the first variant in the app:

```python
response = client.apps.list_app_variants(app_id=app_id)
print(response)
myvariant_id = response[0].variant_id
```

Then, let's start the evaluation jobs:

```python
from agenta.client.types.llm_run_rate_limit import LlmRunRateLimit

rate_limit_config = LlmRunRateLimit(
    batch_size=10,  # number of rows to call in parallel
    max_retries=3,  # max number of time to retry a failed llm call
    retry_delay=2,  # delay before retrying a failed llm call
    delay_between_batches=5,  # delay between batches
)

response = client.evaluations.create_evaluation(
    app_id=app_id,
    variant_ids=[myvariant_id],
    testset_id=test_set_id,
    evaluators_configs=[letter_match_eval_id],
    rate_limit=rate_limit_config
)
print(response)
```

## Checking evaluation status

Now we can check for the status of the job:

```python
client.evaluations.fetch_evaluation_status('667d98fbd1812781f7e3761a')
```

## Configuring rate limits

<!-- TODO: Add more details about rate limit configuration -->

## Handling errors

<!-- TODO: Add content for error handling -->

## Next steps

- Learn about [viewing results](/evaluation/evaluation-from-sdk/viewing-results)
- Explore [advanced evaluation patterns](/evaluation/evaluation-from-sdk/setup-configuration)
