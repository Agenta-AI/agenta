---
title: "Configure Evaluators"
sidebar_label: "Overview"
description: "Set up evaluators for your use case"
sidebar_position: 1
---

import Image from "@theme/IdealImage";

This guide shows you how to configure evaluators for your LLM application.

## Configuring evaluators

To create a new evaluator, click the `Create New` button in the `Evaluators` page.

## Selecting evaluators

Agenta offers a growing list of pre-built evaluators suitable for most use cases. You can also [create custom evaluators](/evaluation/configure-evaluators/custom-evaluator) by writing your own Python function or [use webhooks](/evaluation/configure-evaluators/webhook-evaluator) for evaluation.

<details id="available-evaluators">
<summary>Available Evaluators</summary>

| **Evaluator Name**                                                                                            | **Use Case**                     | **Type**           | **Description**                                                                  |
| ------------------------------------------------------------------------------------------------------------- | -------------------------------- | ------------------ | -------------------------------------------------------------------------------- |
| [Exact Match](/evaluation/configure-evaluators/classification-entity-extraction#exact-match)                | Classification/Entity Extraction | Pattern Matching   | Checks if the output exactly matches the expected result.                        |
| [Contains JSON](/evaluation/configure-evaluators/classification-entity-extraction#contains-json)            | Classification/Entity Extraction | Pattern Matching   | Ensures the output contains valid JSON.                                          |
| [Regex Test](/evaluation/configure-evaluators/regex-evaluator)                          | Classification/Entity Extraction | Pattern Matching   | Checks if the output matches a given regex pattern.                              |
| [JSON Field Match](/evaluation/configure-evaluators/classification-entity-extraction#json-field-match)      | Classification/Entity Extraction | Pattern Matching   | Compares specific fields within JSON data.                                       |
| [JSON Diff Match](/evaluation/configure-evaluators/classification-entity-extraction#json-diff-match)        | Classification/Entity Extraction | Similarity Metrics | Compares generated JSON with a ground truth JSON based on schema or values.      |
| [Similarity Match](/evaluation/configure-evaluators/semantic-similarity#similarity-match)                   | Text Generation / Chatbot        | Similarity Metrics | Compares generated output with expected using Jaccard similarity.                |
| [Semantic Similarity Match](/evaluation/configure-evaluators/semantic-similarity#semantic-similarity-match) | Text Generation / Chatbot        | Semantic Analysis  | Compares the meaning of the generated output with the expected result.           |
| [Starts With](/evaluation/configure-evaluators/regex-evaluator)                                | Text Generation / Chatbot        | Pattern Matching   | Checks if the output starts with a specified prefix.                             |
| [Ends With](/evaluation/configure-evaluators/regex-evaluator)                                    | Text Generation / Chatbot        | Pattern Matching   | Checks if the output ends with a specified suffix.                               |
| [Contains](/evaluation/configure-evaluators/regex-evaluator)                                      | Text Generation / Chatbot        | Pattern Matching   | Checks if the output contains a specific substring.                              |
| [Contains Any](/evaluation/configure-evaluators/regex-evaluator)                              | Text Generation / Chatbot        | Pattern Matching   | Checks if the output contains any of a list of substrings.                       |
| [Contains All](/evaluation/configure-evaluators/regex-evaluator)                              | Text Generation / Chatbot        | Pattern Matching   | Checks if the output contains all of a list of substrings.                       |
| [Levenshtein Distance](/evaluation/configure-evaluators/semantic-similarity#levenshtein-distance)           | Text Generation / Chatbot        | Similarity Metrics | Calculates the Levenshtein distance between output and expected result.          |
| [LLM-as-a-judge](/evaluation/configure-evaluators/llm-as-a-judge)                                           | Text Generation / Chatbot        | LLM-based          | Sends outputs to an LLM model for critique and evaluation.                       |
| [RAG Faithfulness](/evaluation/configure-evaluators/rag-evaluators)                                         | RAG / Text Generation / Chatbot  | LLM-based          | Evaluates if the output is faithful to the retrieved documents in RAG workflows. |
| [RAG Context Relevancy](/evaluation/configure-evaluators/rag-evaluators)                                    | RAG / Text Generation / Chatbot  | LLM-based          | Measures the relevancy of retrieved documents to the given question in RAG.      |
| [Custom Code Evaluation](/evaluation/configure-evaluators/custom-evaluator)                                 | Custom Logic                     | Custom             | Allows users to define their own evaluator in Python.                            |
| [Webhook Evaluator](/evaluation/configure-evaluators/webhook-evaluator)                                     | Custom Logic                     | Custom             | Sends output to a webhook for external evaluation.                               |

</details>

<div style={{ display: "flex", justifyContent: "center" }}>
  <Image
    img={require("/images/evaluation/configure-evaluators-1.png")}
    style={{ width: "50%" }}
    alt="Create new evaluator"
  />
</div>

## Evaluators' playground

Each evaluator comes with its unique playground. For instance, in the screen below, the LLM-as-a-judge evaluator requires you to specify the prompt to use for the evaluation. You'll find detailed information about these parameters on each evaluator's documentation page.

<div style={{ display: "flex", justifyContent: "center" }}>
  <Image
    img={require("/images/evaluation/configure-evaluators-3.png")}
    alt="LLM-as-a-judge evaluator playground"
  />
</div>

The evaluator playground lets you test your evaluator with sample input to make sure it's configured correctly.

To use it, follow these steps:
1. Load a test case from a test set
2. Select a prompt and run it
3. Run the evaluator to see the result

You can adjust the configuration until you are happy with the result. When finished, commit your changes.


## Next steps

Explore the different evaluator types:

- [Classification and Entity Extraction](/evaluation/configure-evaluators/classification-entity-extraction)
- [Pattern Matching](/evaluation/configure-evaluators/regex-evaluator)
- [Semantic Similarity](/evaluation/configure-evaluators/semantic-similarity)
- [LLM as a Judge](/evaluation/configure-evaluators/llm-as-a-judge)
- [RAG Evaluators](/evaluation/configure-evaluators/rag-evaluators)
- [Custom Evaluators](/evaluation/configure-evaluators/custom-evaluator)
- [Webhook Evaluators](/evaluation/configure-evaluators/webhook-evaluator)
