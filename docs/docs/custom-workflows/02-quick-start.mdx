---
title: "Quick Start"
description: "Build your first custom workflow with Agenta"
---

```mdx-code-block
import Image from "@theme/IdealImage";

```

<Image
  style={{ display: "block", margin: "10 auto", marginBottom: "20px" }}
  img={require("/images/custom-workflows/workflow-cop.png")}
  alt="Illustration of the workflow for the chain of prompts application"
  loading="lazy"
/>

In this tutorial, you'll build a custom workflow with two prompts. By the end, you'll have an interactive playground to run and evaluate your chain of prompts.

:::tip
The complete code for this tutorial is available [here](https://github.com/Agenta-AI/agenta/tree/main/examples/python/custom_workflows/chain_of_prompts/).
:::

## What you will build

A chain-of-prompts application that:

1. Takes a blog post as input
2. Summarizes it (first prompt)
3. Writes a tweet from the summary (second prompt)

## 1. Create the application

We will build an app that summarizes a blog post and generates a tweet. The highlighted lines show Agenta integration.

```python title="app.py"
from openai import OpenAI
from pydantic import BaseModel, Field
# highlight-start
import agenta as ag
from agenta.sdk.types import PromptTemplate, Message, ModelConfig
# highlight-end
from opentelemetry.instrumentation.openai import OpenAIInstrumentor

# highlight-next-line
ag.init()
client = OpenAI()
OpenAIInstrumentor().instrument()


# highlight-start
class Config(BaseModel):
    prompt1: PromptTemplate = Field(
        default=PromptTemplate(
            messages=[
                Message(role="system", content="You summarize blog posts concisely."),
                Message(role="user", content="Summarize this:\n\n{{blog_post}}")
            ],
            template_format="curly",
            input_keys=["blog_post"],
            llm_config=ModelConfig(model="gpt-4o-mini", temperature=0.7)
        )
    )
    prompt2: PromptTemplate = Field(
        default=PromptTemplate(
            messages=[
                Message(role="user", content="Write a tweet based on this:\n\n{{summary}}")
            ],
            template_format="curly",
            input_keys=["summary"],
            llm_config=ModelConfig(model="gpt-4o-mini", temperature=0.9)
        )
    )
# highlight-end


# highlight-next-line
@ag.route("/", config_schema=Config)
@ag.instrument()
async def generate(blog_post: str) -> str:
    # highlight-next-line
    config = ag.ConfigManager.get_from_route(schema=Config)

    # Step 1: Summarize
    # highlight-start
    formatted1 = config.prompt1.format(blog_post=blog_post)
    response1 = client.chat.completions.create(**formatted1.to_openai_kwargs())
    # highlight-end
    summary = response1.choices[0].message.content

    # Step 2: Write tweet
    formatted2 = config.prompt2.format(summary=summary)
    response2 = client.chat.completions.create(**formatted2.to_openai_kwargs())

    return response2.choices[0].message.content
```

Let's explore each section:

### Initialization

```python
import agenta as ag
ag.init()
```

Initialize Agenta using `ag.init()`. This sets up the connection to Agenta's backend.

### Configuration with PromptTemplate

```python
class Config(BaseModel):
    prompt1: PromptTemplate = Field(default=PromptTemplate(...))
```

`PromptTemplate` bundles everything needed for an LLM call: messages, model, temperature, and other settings. Agenta renders a rich editor for each `PromptTemplate` field in the playground.

:::note
Use `{{variable}}` syntax with `template_format="curly"`. The `input_keys` list tells Agenta which variables to expect.
:::

### Entry point

```python
@ag.route("/", config_schema=Config)
async def generate(blog_post: str) -> str:
```

The `@ag.route` decorator exposes your function to Agenta. The `config_schema` parameter tells Agenta what configuration to show in the playground.

### Accessing configuration

```python
config = ag.ConfigManager.get_from_route(schema=Config)
```

This retrieves the configuration from the current request. When you edit prompts in the playground, the new values arrive here.

### Using PromptTemplate

```python
formatted = config.prompt1.format(blog_post=blog_post)
response = client.chat.completions.create(**formatted.to_openai_kwargs())
```

`format()` substitutes variables and returns a new template. `to_openai_kwargs()` converts it to arguments for the OpenAI client.

:::warning
The `@ag.instrument()` decorator must come after `@ag.route()`.
:::

## 2. Create the entry point

```python title="main.py"
from dotenv import load_dotenv
load_dotenv()

import app  # noqa: F401 - registers routes

from agenta import app as application
```

This file loads environment variables and exports the FastAPI app for uvicorn.

## 3. Add dependencies

```text title="requirements.txt"
agenta
openai
python-dotenv
opentelemetry-instrumentation-openai
```

## 4. Set environment variables

```bash title=".env"
OPENAI_API_KEY=sk-...
AGENTA_API_KEY=ag-...
AGENTA_HOST=https://cloud.agenta.ai
```

## 5. Run and connect

Start your server:

```bash
uvicorn main:application --host 0.0.0.0 --port 8000 --reload
```

Expose it with ngrok:

```bash
ngrok http 8000
```

:::warning
Your server must be accessible from the internet for evaluations to work. Use ngrok or deploy to a public URL.
:::

Copy the ngrok URL (e.g., `https://abc123.ngrok.io`). In the Agenta UI, click **"Create Custom Workflow"** and provide this URL.

<Image
  style={{ display: "block", margin: "10 auto" }}
  img={require("/images/custom-workflows/custom-workflow-modal.png")}
  alt="Screenshot of the modal for creating a custom workflow in Agenta"
  loading="lazy"
/>

## Using the playground

Once connected, you can:

- Edit prompts in the UI
- Change model parameters
- Test with different inputs
- Save new variants
- Compare versions side by side

The traces panel shows each step of your workflow, including LLM calls and their inputs/outputs.

<Image
  style={{ display: "block", margin: "10 auto" }}
  img={require("/images/custom-workflows/trace-cop.png")}
  alt="Screenshot of playground traces for chain of prompts application"
  loading="lazy"
/>

## Next steps

- [Integration](/custom-workflows/integration): How Agenta integrates with your service in dev and prod
- [Observability](/observability/overview): Add detailed tracing to your workflow
