---
title: "Manage Prompts with SDK"
---

import Image from "@theme/IdealImage";
import GoogleColabButton from "@site/src/components/GoogleColabButton";

You can manage prompts easily through Agenta's web UI. But sometimes you might want to do things programmatically instead of using the interface.

In this tutorial, we'll use the Agenta SDK to create a new prompt, commit changes, deploy them to production, then fetch their configuration and use it to call the LLM.

<GoogleColabButton notebookPath="examples/jupyter/prompt-management/manage-prompts-with-sdk-tutorial.ipynb">
  Open in Google Colaboratory
</GoogleColabButton>

## Tutorial Overview

Before we begin, let's quickly review how Agenta versions prompts:

Agenta follows a structure similar to **git** for prompt versioning. Instead of having one commit history, it uses **multiple branches (called variants)** where changes can be committed, and **environments** where these changes can be deployed (and used in your application). (You can read more about why we chose this approach [here](/concepts/concepts#motivation)).
You can find more about how prompt versioning works in the [concepts page](/prompt-engineering/concepts).

The workflow for deploying a change to production that we'll follow in this tutorial is:

1. Create a new variant
2. Commit a change to that variant
3. Deploy that commit (variant/version) to the production environment
4. Commit a new change to that variant
5. Fetch the config from that environment

## Setup

Before using the SDK, we need to initialize it using the `ag.init()` method, which takes the host (default `cloud.agenta.ai`) and the API key (not required for community editions):

```python
import os
import agenta as ag

# Initialize the SDK with your API key
os.environ["AGENTA_API_KEY"] = "xxx" # Only needs setting in OSS
os.environ["AGENTA_HOST"] = "https://cloud.agenta.ai" # Default value, no need to set explicitly
ag.init()
```

## Creating a new prompt

We're going to create a new completion prompt called `topic-explainer`.


```Python
# Creates an empty application
app = ag.AppManager.create(
    app_slug="topic-explainer",
    template_key="SERVICE:completion", # we define here the app type, choices are SERVICE:completion, SERVICE:chat, CUSTOM
)
```
:::warning
The app created until now is empty. You cannot use it from the UI yet. You need to create a variant and commit changes to it to be able to use it (next section).
:::

## Creating a new variant

Variants are similar to branches in **git**. Any change to the prompt must first be committed to a variant. Here, we'll create a new variant and make our first commit to it using the `VariantManager.create` method:

```python
from agenta.sdk.types import PromptTemplate, Message, ModelConfig
from pydantic import BaseModel

# We need to create a Pydantic Model with a `prompt` field of type `PromptTemplate`
class Config(BaseModel):
    prompt: PromptTemplate

config = Config(
    prompt=PromptTemplate(
        messages=[
            Message(role="system", content="You are an assistant that provides concise answers"),
            Message(role="user", content="Explain {{topic}} in simple terms"),
        ],
        llm_config=ModelConfig(
            model="gpt-3.5-turbo",
            max_tokens=150,
            temperature=0.6,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0,
        ),
        template_format="curly"
    )
)

# Create a new variant
variant = ag.VariantManager.create(
    parameters=config.model_dump(),
    app_slug="topic-explainer",
    variant_slug="new-variant"
)

print("Created variant:")
print(variant)
```

This command will create a new variant and initialize it with the first commit containing the provided parameters.

- **Parameters:**
  - `app_slug`: The unique slug of your application.
  - `variant_slug`: The unique slug of the new variant.
  - `parameters`: A dictionary containing the initial configuration parameters.

**Sample Output:**

```python
Created variant:
{
    "app_id": "01963413-3d39-7650-80ce-3ad5d688da6c",
    "app_slug": "topic-explainer",
    "variant_id": "01968c11-6f7c-7773-b273-922c5807be7b",
    "variant_slug": "new-variant",
    "variant_version": 1,
    "environment_id": null,
    "environment_slug": null,
    "environment_version": null,
    "committed_at": "2025-05-01T07:26:08.935406+00:00",
    "committed_by": "user@agenta.ai",
    "committed_by_id": "0196247a-ec9d-7051-8880-d58279570aa1",
    "deployed_at": null,
    "deployed_by": null,
    "deployed_by_id": null,
    "params": {
        "prompt": {
            "messages": [
                {
                    "name": null,
                    "role": "system",
                    "content": "You are an assistant that provides concise answers",
                    "tool_calls": null,
                    "tool_call_id": null
                },
                {
                    "name": null,
                    "role": "user",
                    "content": "Explain {{topic}} in simple terms",
                    "tool_calls": null,
                    "tool_call_id": null
                }
            ],
            "input_keys": null,
            "llm_config": {
                "model": "gpt-3.5-turbo",
                "tools": null,
                "top_p": 1.0,
                "stream": null,
                "max_tokens": 150,
                "temperature": 0.6,
                "tool_choice": null,
                "response_format": null,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0
            },
            "user_prompt": null,
            "system_prompt": null,
            "template_format": "curly"
        }
    }
}
```

## Deploying changes to the production environment

[Todo: Image showing before and after. The new elements are added in a different colors]

To deploy our commit to an environment, use the `DeploymentManager.deploy` method.

```python
# Deploy the variant to the production environment
deployment = ag.DeploymentManager.deploy(
    app_slug="topic-explainer",
    variant_slug="new-variant",
    environment_slug="production",
)

print("Deployed variant to environment:")
print(deployment)
```

- **Parameters:**

  - `environment_slug`: The slug of the environment (`development`, `staging`, or `production`).

- **Notes:**
  - Deploying a variant without specifying a `variant_version` deploys the latest version.

**Sample Output:**

```python
Deployed variant to environment:
{
    "app_id": "01963413-3d39-7650-80ce-3ad5d688da6c",
    "app_slug": "topic-explainer",
    "variant_id": "01968c11-6f7c-7773-b273-922c5807be7b",
    "variant_slug": "new-variant",
    "variant_version": 1,
    "environment_id": "01968c14-c35d-7440-bcc8-9def594f017f",
    "environment_slug": "production",
    "environment_version": 1,
    "committed_at": "2025-05-01T07:26:08.935406+00:00",
    "committed_by": "user@agenta.ai",
    "committed_by_id": "0196247a-ec9d-7051-8880-d58279570aa1",
    "deployed_at": "2025-05-01T13:41:33.149595+00:00",
    "deployed_by": "user@agenta.ai",
    "deployed_by_id": "0196247a-ec9d-7051-8880-d58279570aa1",
    "params": {
        "prompt": {
            "messages": [
                {
                    "name": null,
                    "role": "system",
                    "content": "You are an assistant that provides concise answers",
                    "tool_calls": null,
                    "tool_call_id": null
                },
                {
                    "name": null,
                    "role": "user",
                    "content": "Explain {{topic}} in simple terms",
                    "tool_calls": null,
                    "tool_call_id": null
                }
            ],
            "input_keys": null,
            "llm_config": {
                "model": "gpt-3.5-turbo",
                "tools": null,
                "top_p": 1.0,
                "stream": null,
                "max_tokens": 150,
                "temperature": 0.6,
                "tool_choice": null,
                "response_format": null,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0
            },
            "user_prompt": null,
            "system_prompt": null,
            "template_format": "curly"
        }
    }
}
```

## Committing a change to variant

[ Image showing changes]

We're now going to commit changes to our variant. Note that this will not modify the version in deployment!

To save changes to a variant (creating a new version), we are going to use the `VariantManager.commit` method with explicit parameters.

```python
config2 = Config(
    prompt=PromptTemplate(
        messages=[
            Message(role="system", content="You are an assistant that provides concise answers"),
            Message(role="user", content="Use Paul Graham style to explain {{topic}} in simple terms."),
        ],
        llm_config=ModelConfig(
            model="gpt-3.5-turbo",
            max_tokens=150,
            temperature=0.9,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0,
        ),
        template_format="curly"
    )
)

# Commit the new version
variant = ag.VariantManager.commit(
    parameters=config2.model_dump(),
    app_slug="topic-explainer",
    variant_slug="new-variant"
)

print("Committed new version of variant:")
print(variant)
```

:::info Immutability
Each commit creates a new version of the variant. Versions are immutable once created.
:::

**Sample Output:**

```python
Committed new version of variant:
{
    "app_id": "01963413-3d39-7650-80ce-3ad5d688da6c",
    "app_slug": "topic-explainer",
    "variant_id": "01968c11-6f7c-7773-b273-922c5807be7b",
    "variant_slug": "new-variant",
    "variant_version": 2,
    "environment_id": null,
    "environment_slug": null,
    "environment_version": null,
    "committed_at": "2025-05-01T07:26:08.935406+00:00",
    "committed_by": "user@agenta.ai",
    "committed_by_id": "0196247a-ec9d-7051-8880-d58279570aa1",
    "deployed_at": null,
    "deployed_by": null,
    "deployed_by_id": null,
    "params": {
        "prompt": {
            "messages": [
                {
                    "name": null,
                    "role": "system",
                    "content": "You are an assistant that provides concise answers",
                    "tool_calls": null,
                    "tool_call_id": null
                },
                {
                    "name": null,
                    "role": "user",
                    "content": "Use Paul Graham style to explain {{topic}} in simple terms.",
                    "tool_calls": null,
                    "tool_call_id": null
                }
            ],
            "input_keys": null,
            "llm_config": {
                "model": "gpt-3.5-turbo",
                "tools": null,
                "top_p": 1.0,
                "stream": null,
                "max_tokens": 150,
                "temperature": 0.9,
                "tool_choice": null,
                "response_format": null,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0
            },
            "user_prompt": null,
            "system_prompt": null,
            "template_format": "curly"
        }
    }
}
```

## Fetching the prompt in production

Now we'll fetch and use the prompt that's in production. Keep in mind that the production environment still references the first version of our variant. If we want it to reflect the latest changes, we'll need to deploy it again.

```python
# Fetch configuration from the production environment
config = ag.ConfigManager.get_from_registry(
    app_slug="topic-explainer",
    environment_slug="production"
)

print("Fetched configuration from production:")
print(config)

# Using the configuration with OpenAI client
from openai import OpenAI

# Format the prompt with your topic
topic = "artificial intelligence"
prompt = PromptTemplate(**config["prompt"]).format(topic=topic)
client = OpenAI()

response = client.chat.completions.create(
    **prompt.to_openai_kwargs()
)

print(response.choices[0].message.content)
```

## Next Steps

Now that you've learned how to manage configurations using the SDK, you can:

- Read the guide to explore more advanced features of the SDK. [add link]
- Read how to reference prompts in your traces. [add link]
- Read how to manage configuration for your workflows (chain of prompts, RAG..). [add link]
