---
title: "Adding Custom LLM Providers"
description: "Learn how to add and configure custom LLM providers like Azure OpenAI, AWS Bedrock, Ollama, and others to use your own models in Agenta's playground and applications."
sidebar_position: 3
---


```mdx-code-block
import { Stream } from '@cloudflare/stream-react';
import Image from "@theme/IdealImage";
```

Agenta supports a wide range of LLM providers beyond the default options. This guide shows you how to configure custom providers and models for use in your Agenta applications.

You can integrate self-hosted models into Agenta by adding custom providers such as:

- [Azure OpenAI](#configuring-azure-openai)
- [AWS Bedrock](#configuring-aws-bedrock)
- [Vertex AI](#configuring-vertex-ai)
- [OpenAI-compatible endpoints](#configuring-openai-compatible-endpoints-eg-ollama) (including self-hosted models, OpenAI finetuned models, [Ollama](#configuring-openai-compatible-endpoints-eg-ollama), etc.)


:::info
You can add custom models to any of the providers already listed in the playground, such as [OpenRouter](#adding-models-to-a-provider-eg-openrouter), Anthropic, Gemini, Cohere, and others. This is especially useful for accessing specialized or new models that aren't included in Agenta's default configurations.
:::

---

## How to Add Custom LLM Providers

<Stream controls src="acabccbcc814dc051a54bf2c97ff097c" height="400px" />
<br />

### From Settings

1. Navigate to **Settings** → **Model Hub**.  
2. Click **Create Custom Provider**.  
3. Select the provider type and enter the required credentials.  
4. Click **Save**.  


### From the Playground

1. Open any app in the Playground.  
2. Click the model dropdown menu.  
3. Select **Add Custom Provider**.  

:::note
You can add multiple models to the same provider. This allows you to organize related models under a single provider configuration.
:::

## Configuring Vertex AI

To add Vertex AI models, please provide:

- **Vertex Project**: Your Google Cloud **project ID** (e.g., `my-project-123`)
- **Vertex Location**: The **region** (e.g., `europe-west4`, `us-central1`)
- **Vertex Credentials**: A **Service Account** key in **JSON** format

### How to Retrieve Vertex Credentials

Follow Google’s guides to authenticate and create a service account key:

- Vertex AI authentication: [https://cloud.google.com/vertex-ai/docs/authentication](https://cloud.google.com/vertex-ai/docs/authentication)  
- Create / manage service account keys: [https://cloud.google.com/iam/docs/keys-create-delete#iam-service-account-keys-create-console](https://cloud.google.com/iam/docs/keys-create-delete#iam-service-account-keys-create-console)

**Steps to Retrieve Vertex Credentials in the Console**:

1. In the Google Cloud console, go to **IAM & Admin → Service accounts**.  
2. Select a **Project**.
3. Click the email address of the service account that you want to create a key for.
4. Click the **Keys** tab.
5. Click the **Add key** drop-down menu, then select **Create new key**.
6. Select **JSON** as the **Key type** and click **Create**.

> Clicking **Create** downloads a service account key file. After you download the key file, you cannot download it again.

### Configuration Example

```plaintext
Vertex project: my-project-123
Vertex location: europe-west4
Vertex credentials:
{
  "type": "service_account",
  "project_id": "PROJECT_ID",
  "private_key_id": "KEY_ID",
  "private_key": "-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n",
  "client_email": "SERVICE_ACCOUNT_EMAIL",
  "client_id": "CLIENT_ID",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://accounts.google.com/o/oauth2/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL"
}
```

### Available models

See the LiteLLM docs for an up-to-date list of models:

- Vertex Gemini models: [https://docs.litellm.ai/docs/providers/vertex](https://docs.litellm.ai/docs/providers/vertex)  
- Vertex partner models: [https://docs.litellm.ai/docs/providers/vertex_partner](https://docs.litellm.ai/docs/providers/vertex_partner)  
- Vertex self-deployed models: [https://docs.litellm.ai/docs/providers/vertex_self_deployed](https://docs.litellm.ai/docs/providers/vertex_self_deployed)

:::warning
**Do not include** the `vertex_ai/` prefix in the **model name**. Use the bare model string (e.g., `gemini-2.5-pro`, `claude-3-5-sonnet@20240620`).
:::

## Configuring Azure OpenAI

To add Azure OpenAI models, you'll need the following information:

- **API Key**: Your Azure OpenAI API key  
- **API Version**: The API version (e.g., `2023-05-15`)  
- **Endpoint url**: The endpoint of your Azure resource  
- **Deployment Name**: The name of your model deployment  

### How to Retrieve Azure Credentials

1. **Access the Azure Portal:**  
  Log in to the [Azure Portal](https://portal.azure.com).  

2. **Locate Azure OpenAI Service:**  
  - Search for **Azure AI Services** in the portal.  
  - Click on your resource to view its details.  

3. **Retrieve API Keys and Endpoints:**  
  - Navigate to the **Keys and Endpoint** section.  
  - Copy the API key and endpoint URL.  

4. **Find Deployment Names:**  
  - Go to **Model Deployments**.  
  - Select the desired deployment and note its name.  

### Configuration Example

```plaintext
API Key: xxxxxxxxxx
API Version: 2023-05-15
API base url: Use here your endpoint URL (e.g., https://accountnameinstance.openai.azure.com
Deployment Name: Use here the deployment name in Azure (e.g., gpt-4-turbo)
```

## Configuring AWS Bedrock

To add AWS Bedrock models, you'll need:

- **Access Key ID:** Your AWS access key
- **Secret Access Key:** Your AWS secret key
- **Region:** The region where your Bedrock models are deployed

### How to Retrieve AWS Credentials

Refer to these tutorials for detailed instructions:
  - [AWS Bedrock Getting Started Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html)
  - [DataCamp AWS Bedrock Tutorial](https://www.datacamp.com/tutorial/aws-bedrock)


### Configuration Example

```plaintext
Access Key ID: xxxxxxxxxx
Secret Access Key: xxxxxxxxxx
Region: <region_name> (e.g eu-central-1)
Model name: <model_name> (e.g anthropic.claude-3-sonnet-20240229-v1:0)
```


## Configuring OpenAI-Compatible Endpoints (e.g., Ollama)

For any OpenAI-compatible API, including self-hosted models:

- **API Key:** The API authentication key
- **Base URL:** The API endpoint URL

This configuration works for:
- Self-hosted models using vLLM
- LocalAI
- 3rd party providers with OpenAI-compatible endpoints
- Fine-tuned OpenAI models

### Configuration Example

```
API Key: your-api-key
Base URL: https://your-api-endpoint.com/v1
```

:::warning
Make sure to include `/v1` at the end of the Base URL.
:::

:::warning
If you're running both Ollama and self-hosting Agenta on your local machine, set the Base URL to http://host.docker.internal:11434/v1 to allow Agenta's completion service (running behind Docker) to access Ollama.
:::

## Adding Models to a Provider (e.g. OpenRouter)

For some providers like OpenRouter, Agenta does not include all available models due to their high number. To add models to an existing provider:

1. Navigate to **Settings** → **Model Hub**.  
2. Select the provider you want to add models to.
3. Enter the API key and an identifier
4. Add the models you want to use 
5. Click **Save**  
