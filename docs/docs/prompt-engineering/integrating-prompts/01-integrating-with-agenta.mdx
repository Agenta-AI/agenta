---
title: "How to Integrate with Agenta"
description: "Integrate applications and prompts created in Agenta into your projects."
---

import Image from "@theme/IdealImage";

Agenta integrates with your workflow. You can use the latest version of your deployed prompt in your application. With Agenta, **you can update prompts directly from the web interface without modifying your code** each time.

Here are two ways to use prompts from Agenta in your code:

### [1. As a prompt management system](/prompt-engineering/integrating-prompts/fetch-prompt-programatically)

Prompts are managed and stored in the Agenta backend. You use the Agenta SDK to fetch the latest deployed version of your prompt. Then you use it in your application.

**Advantages**:

- Agenta operates outside your application's critical path.
- You can fetch and cache the latest prompt version for zero latency usage.

**Considerations**:

- You need to set up [observability integration](/observability/quickstart-python) yourself. This is required if you want to trace your calls for debugging and cost tracking.

<Image
  class="bg-white"
  img={require("/images/prompt_management/as-a-prompt-management.png")}
  loading="lazy"
  alt="A sequence diagram showing how to integrate with Agenta as a prompt management system"
/>



### [2. As a middleware/gateway (invoking prompt)](/prompt-engineering/integrating-prompts/proxy-calls)

You invoke your prompts directly through Agenta. Agenta provides you with an endpoint that forwards requests to the LLM on your behalf.

**Advantages**:

- Simplified deployment.
- Automatic tracing without any code changes.

**Considerations**:

- Adds slight latency to the response (approximately 0.3 seconds).
- Streaming is not supported for these endpoints.

This approach works best for applications where latency is not critical.

<Image
  class="bg-white"
  img={require("/images/prompt_management/as-a-proxy.png")}
  loading="lazy"
  alt="A sequence diagram showing how to integrate with Agenta as    a proxy"
/>
