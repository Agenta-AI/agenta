---
title: Logging LLM requests
description: Log discrete events in your application from LLM requests/chains to functions.
---

When building an application from the Agenta UI, you have **tracing** is enabled by default but building a custom application from code, you will have to
configure Agenta to capture all **inputs**, **outputs**, and other **metadata** from your LLM applications, regardless of whether they are hosted in Agenta or in your environment.

All captured metrics are recorded on a dashboard that offers an overview of your app's performance metrics over time after instrumentation.

Agenta also also provide a table detailing all the requests made to your LLM application. This table can be filtered and used to enrich your test sets, debug your applications, or fine-tune them.

**Note: You need to install the Agenta SDK on your computer.**

```python
pip install agenta
```

## 1. Create an application in Agenta

The first steo is to create an application in **Agenta**. You can do this using the command from the CLI:

```python
agenta init
```

Whhat this command does is that it creates a new application in **Agenta** and a `config.toml` file with all the necessary information about the application.

## 2. Initialize Agenta

After creating your application, you have to initialize it and this can be done in two ways.

**Option 1**: Import the **Agenta** package into your code, the **Agenta API key** which can be found in **Settings**.

**Option 2**: You can also add the `app id` which can be found in the `config.toml` file if you have created the application from the CLI.

```python
import agenta as ag
# Option 1

ag.init(api_key="", app_id="")

# Option 2
os.environ["AGENTA_API_KEY"] = ""
os.environ["AGENTA_APP_ID"] = ""
ag.init()

# Option 3
ag.init(config_fname="config.toml") # using the config.toml generated by agenta init

```

<Note>
  Note that if you are serving your application to the **Agenta** cloud,
  **Agenta** will automatically populate all the information in the environment
  variable. Therefore, you only need to use `ag.init()`.
</Note>

## 3. Instrument with the decorator

Add the `@ag.instrument()` decorator to the functions you want to monitor.
This decorator decorator will trace all **input** and **output** information for the functions, helping you keep track of your LLM application's behavior.

Here's a key point to remember: ensure that the `instrument` decorator should always be applied on the top of your function. This guarantees that the logging process is correctly initiated.

```python
@ag.instrument(spankind="llm")
def myllmcall(country:str):
     prompt = f"What is the capital of {country}"
     response = client.chat.completions.create(
        model='gpt-4',
        messages=[
            {'role': 'user', 'content': prompt},
        ],
	)
     return response.choices[0].text

@ag.instrument()
def generate(country:str):
     return myllmcall(country)

```

## 4. Modify a span's metadata

Similarly, you can modify a span's metadata to add additional information using `ag.tracing.set_span_attributes()`. This function will access the active span and add the key-value pairs to the metadata.

```python
@ag.instrument(spankind="llm")
def myllmcall(country:str):
     prompt = f"What is the capital of {country}"
     response = client.chat.completions.create(
        model='gpt-4',
        messages=[
            {'role': 'user', 'content': prompt},
        ],
	)
     ag.tracing.set_span_attributes({"model": "gpt-4"})
     return response.choices[0].text

```

## Putting it all together

Combining the code in steps above will give us this:

```python
import agenta as ag

os.environ["AGENTA_API_KEY"] = ""
os.environ["AGENTA_APP_ID"] = ""
ag.init()

@ag.instrument(spankind="llm")
def myllmcall(country:str):
     prompt = f"What is the capital of {country}"
     response = client.chat.completions.create(
        model='gpt-4',
        messages=[
            {'role': 'user', 'content': prompt},
        ],
	)
     ag.tracing.set_span_attributes({"model": "gpt-4"})
     return response.choices[0].text

@ag.instrument()
def generate(country:str):
     return myllmcall(country)

```

## Setting up telemetry for apps hosted in Agenta

Finally, if you're creating an application to serve to agenta, there is no much chnages to be done.
The only thing to do is to add the `@ag.entrypoint` entrypoint decorator, ensuring it comes **before** the instrument decorator.

```python
import agenta as ag

ag.init()
ag.config.register_default(prompt=ag.TextParam("What is the capital of {country}"))

@ag.instrument(spankind="llm")
def myllmcall(country:str):
     response = client.chat.completions.create(
        model='gpt-4',
        messages=[
            {'role': 'user', 'content': ag.config.prompt.format(country=country)},
        ],
	)
     ag.tracing.set_span_attributes({"model": "gpt-4"})
     return response.choices[0].text

@ag.entrypoint
@ag.instrument()
def generate(country:str):
     return myllmcall(country)

```

You can now proceed to serve your application using `agenta variant serve myapp.py` in the terminal.
The major advantage of this approach is that the configuration you use is automatically instrumented along with the other data.
