---
title: Writing your first LLM app using Langchain
---

This tutorial guides you through writing of your first LLM app using Agenta. The objective is to create an app that can produce a persuasive startup pitch, using the startup's name and core idea. By the end of this tutorial, your app will be set up locally and ready for testing and refinement in the playground.

## Prerequisites

This guide assumes you have completed the installation process. If not, please follow our [installation guide](/installation).

## 1. Project Initialization

First, create an empty project in a new directory.

```bash
mkdir my-first-app; cd my-first-app
agenta init
```

Follow the prompts to initialize your project. Make sure to select `start with an empty project` when prompted.

Now create a virtual environment:

```bash
python3 -m venv env
source env/bin/activate
```

Create a `requirements.txt` file and list the dependencies:

```
langchain
agenta
openai
```

Then install them:
```bash
pip install -r requirements.txt
```

Your project is now ready for development.

## 2. Write a Simple LLM App

Create a file named `app.py`. This will house a simple script that uses the langchain library to generate a startup pitch:

```python

from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

startup_name = "agenta"
startup_idea = "the open-source developer-first llmops platform"

default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""

#template is a float, it is used to decide what sampling temperature to use. Default to 0.7
temperature = 0.5

llm = OpenAI(temperature=temperature)
prompt = PromptTemplate(
    input_variables=["startup_name", "startup_idea"],
    template=prompt_template)

chain = LLMChain(llm=llm, prompt=prompt)
output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
print(output)
```

## 3. Add the Agenta SDK

The Agenta SDK transforms your code into an LLM app deployable in Agenta. The resulting code below illustrates how to integrate the SDK:

```python
import agenta
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""

@ag.post
def generate(
    startup_name: str,
    startup_idea: str,
    prompt_template: ag.TextParam = default_prompt,
    temperature: ag.FloatParam = 0.5,
) -> str:
    llm = OpenAI(temperature=temperature)
    prompt = PromptTemplate(
        input_variables=["startup_name", "startup_idea"],
        template=prompt_template)

    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
    return output
```

Let's examine how we modified the original code by encapsulating it within a function and further decorating it with @ag.post. This critical transformation serves two primary purposes:


```python
@ag.post
def generate(
    startup_name: str,
    startup_idea: str,
    prompt_template: ag.TextParam = default_prompt,
    temperature: ag.FloatParam = 0.5)
```

#### Main Function Designation:
By using the `@ag.post` decorator (a Python feature that adds functionality to functions or methods), we signal to Agenta that this specific function acts as the app's main entry point.

#### Parameter Typing:
The function's parameters play key roles:
  - **Inputs**: The `str` type designates inputs for the app.
  - **Configuration Parameters**: Types like `ag.TextParam`, `ag.FloatParam`, `ag.IntParam`, and `ag.MultiChoiceParam` designate the app's configuration, we can later modify these in the playground. 


## 4. API Key Integration

For using OpenAI or other APIs, store your keys in a .env file:
```bash
OPENAI_API_KEY=sk-xxxxxxx
```


## 5. Run the app in CLI

Use the following command to execute the app via Command Line Interface (CLI):

```bash
python app.py agenta "the open-source developer-first llmops platform"

Hi there, I'm excited to tell you about Agenta, the open source Dev First LLMOPS Platform. It's a revolutionary way to simplify and streamline the development process, and I'm confident it will be a game changer in the industry. Let's chat soon!
```

## 6. Deploying to Agenta

Our app is still local. To make it available in the playground, add it to Agenta by running:

```bash
agenta variant serve --file_name app.py
```

Now you can interact with the app through the API or experiment in the playground.

## 7. Iterative Improvement in the Playground

Your app is now accessible in the Agenta UI at http://localhost:3000. Interact with your app, experiment with different parameters, and fine-tune your app's behavior to achieve desired results.


<Check> Well done! You've created and deployed your first LLM app with Agenta. Continue to experiment and refine your app as needed in the playground. </Check>