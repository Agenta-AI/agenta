---
title: Writing your first LLM app
description: Crafting Your First LLM Application with Agenta
---

In this tutorial, we'll lead you through the process of creating your first Language Learning Model (LLM) application using Agenta. Our aim is to build an application capable of producing a compelling startup pitch based on the startup's name and core idea. By the end of the tutorial, we'll have our app set up locally, ready for iterative refinement in the playground. You'll have the ability to modify parameters, branch out new variants, and systematically assess different versions in Agenta.

Let's begin.

## Prerequisites

This guide assumes you have completed the installation process. If not, please follow our [installation guide](/docs/installation).

## 1. Project Initialization

Start by creating an empty project in an empty directory.

```bash
mkdir my-first-app; cd my-first-app
agenta init
```

Follow the prompts to initialize your project. Make sure to select `start with an empty project` when prompted.

Now create a virtual environment:

```bash
python3 -m venv env
source env/bin/activate
```

Finally let's create a requirements.txt and add within it our dependencies

```
langchain
agenta
python-dotenv
openai
```

Install the dependencies:

```bash
pip install -r requirements.txt
```

Now we're ready to start writing our app.


## 2. Write a Simple LLM App


Create a file called `app.py` - the main script that Agenta SDK expects. This simple script takes a startup name and idea, and generates a pitch using langchain:


```python

from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

startup_name = "agenta"
startup_idea = "the open-source developer-first llmops platform"

default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""

llm = OpenAI(temperature=temperature)
prompt = PromptTemplate(
    input_variables=["startup_name", "startup_idea"],
    template=prompt_template)

chain = LLMChain(llm=llm, prompt=prompt)
output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
print(output)
```

## 3. Encapsulate Your Code in a Function

Wrap your code into a function named `generate`. This function will be the primary function Agenta calls, taking in strings as inputs and returning a string as an output:

```python
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate


default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""


def generate(startup_name, startup_idea):
    llm = OpenAI(temperature=temperature)
    prompt = PromptTemplate(
        input_variables=["startup_name", "startup_idea"],
        template=prompt_template)

    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
    return output

```

## 4. Add the decorator

We'll now transform our plain Python function into an Agenta app. Use the Agenta decorator `@post` for this purpose. The decorator tells Agenta that this function is the main function in the app, and it specifies the inputs (and later parameters) that the app takes. 

Note here that we made sure to add the type annotations to the function arguments. This is required for Agenta to be able to understand the inputs and outputs of the function.

```python

from agenta import post
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
import os


default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""


@post
def generate(startup_name: str, startup_idea: str) -> str:
    llm = OpenAI(temperature=temperature)
    prompt = PromptTemplate(
        input_variables=["startup_name", "startup_idea"],
        template=prompt_template)

    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
    return output
```


## 5. Parameter Addition

To iterate and fine-tune this app in the playground, we need to expose parameters. Accomplish this by adding additional arguments in the app function:


```python
from agenta import post, TextParam, FloatParam
from dotenv import load_dotenv
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
import os


default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""


@post
def generate(startup_name: str, startup_idea: str, prompt_template: TextParam = default_prompt, temperature: FloatParam = 0.5) -> str:
    llm = OpenAI(temperature=temperature)
    prompt = PromptTemplate(
        input_variables=["startup_name", "startup_idea"],
        template=prompt_template)

    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
    return output
```

In this example, `startup_name` and `startup_idea` are required inputs, while `prompt` and `temperature` are optional parameters with default values. The types TextParam and FloatParam are used to indicate these are parameters rather than inputs, and they will be made available for tuning in the Agenta UI.

## 6. API Key Integration

To use openai or other APIs, you'll need appropriate keys.  Create an empty `.env` file and add your keys to it. Then, read the keys from the environment in your code:

```bash
OPENAI_API_KEY=sk-xxxxxxx
````
We just need to read the keys from the environement and use them in our code.

```python
from agenta import post, TextParam, FloatParam
from dotenv import load_dotenv
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
import os


default_prompt = """
    please write a short linkedin message (2 SENTENCES MAX) to an investor pitching the following startup:
    startup name: {startup_name}
    startup idea: {startup_idea}"""


@post
def generate(startup_name: str, startup_idea: str, prompt_template: TextParam = default_prompt, temperature: FloatParam = 0.5) -> str:
    load_dotenv()
    llm = OpenAI(temperature=temperature)
    prompt = PromptTemplate(
        input_variables=["startup_name", "startup_idea"],
        template=prompt_template)

    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(startup_name=startup_name, startup_idea=startup_idea)
    return output

````

## 7. Run the code in CLI
The `@post` decorator allows the function to run in the Command Line Interface (CLI). Execute it using the command:

```bash
python app.py agenta "the open-source developer-first llmops platform"

Hi there, I'm excited to tell you about Agenta, the open source Dev First LLMOPS Platform. It's a revolutionary way to simplify and streamline the development process, and I'm confident it will be a game changer in the industry. Let's chat soon!
```

## 8. Deploying to Agenta
Our app is still local. To make it available in the playground, we need to intialize it for deployment by running the following command:

```bash
agenta init
```

You will be asked four questions:

1. The name of your app.
2. Whether you're running your app locally (answer 'yes' or 'no').
3. How you want to initialize your app (provide details on the options available).
4. The template you wish to use (optional).

If you built your app by following the tutorial, proceed to reply to the third question by selecting 'Blank App'. The choice you make in question 3 may impact the availability of the fourth question, so choose accordingly.

Proceed to serve your application which will add it to the backend by running:

```bash
agenta variant serve
```

This commands build a docker image of your app and push it to the Agenta registry. It also creates a new variant for your app in the Agenta UI. You can now go to the Agenta UI and start playing with your app.

## 9. Iterative Improvement in the Playground


Your app is now accessible in the Agenta UI at localhost:3000. Interact with your app, experiment with different parameters, and fine-tune your app's behavior to achieve desired results.

## Conclusion

Congratulations! You've crafted and deployed your first LLM app with Agenta. You can now experiment and refine it further in the playground. 


