---
title: "Retrieval Augumented Generation (RAG)"
---

**Retrieval-Augmented Generation** is an approach that works by retrieving relevant information from external sources and then using that information
to generate more accurate and contextually relevant response. This method enhances the model's ability to produce informed, factually correct, and
context-aware responses, especially when dealing with complex queries or specific knowledge domains.

RAG applications are valuable in scenarios where up-to-date or domain-specific knowledge is important, such as customer support, research assistance, etc. while reducing hallucinations
by ensuring that the generated output is based on retrieved information.

In this guide, you will learn how to configure and use RAG evaluators for your custom-built application. **RAG evaluators** are unique because they need intermediate `outputs` and internal `variables` (such as context) to run.

## Prerequisite

To be able to follow through with this guide, you need to setup the following:

- Vector Database ([MongoDB](https://www.mongodb.com/products/platform/atlas-database))
- LLM provider ([OpenAI](https://platform.openai.com/))
- Agenta [API key](https://cloud.agenta.ai/settings?tab=apiKeys)

### Vectore Database

A **vector database** is a specialized type of database designed to store, index, and search high-dimensional vectors, which are numerical representations of data.
For this guide, we will make use of MongoDB as a vector database. To setup MongoDB, following the steps below:

- Connect to MongoDB [MongoDB](https://www.mongodb.com/products/platform/atlas-database)
- Create a Cluster
- Configure Database Access to access data from username and password
- Configure Network Access to allow for ingress from your IP address
- Load [sample datasets](https://www.mongodb.com/docs/atlas/sample-data/#std-label-load-sample-data), expecially the [mflix dataset](https://www.mongodb.com/docs/atlas/sample-data/sample-mflix/)

## Developing the RAG Application

Let's but a RAG application that assists users in generating movie recommendations based on specific topics and genres.
The application follows a multi-step workflow: it retrieves relevant movies from a database,
generates a detailed report about those movies, and then summarizes the report. The primary goal of this application is to provide
users with a refined and concise list of movie recommendations and summaries based on their preferences.

If you want to skip ahead and get the source code the RAG application, you can find it [here](https://github.com/Agenta-AI/agenta/tree/main/examples/rag_applications/mflix).

<Note>
  {" "}

- Access to internal variables (`internals`) and intermediate outputs (`traces` or `inline traces`) requires version `0.20.0+` of the Python SDK.
- Access to RAG Evaluators and `view trace` in the Web UI requires Agenta Cloud and is not available in Agenta OSS. {" "}

</Note>

Let's start by setting up pur environement and importing the necessary tools for the application to function.
Each `import` plays a specific role in ensuring the application runs smoothly and securely.

```python
import certifi
from dotenv import dotenv_values
from pymongo import MongoClient
from pymongo.server_api import ServerApi
from openai import OpenAI

import agenta as ag
```

Let's also setup the application's connection to two key services: OpenAI for the LLM tasks and MongoDB for vectore data storage.
By loading environment variables securely from a `.env` file, the code initializes the OpenAI `client` with an API key and establishes a secure connection to a MongoDB `database` using the provided URI.

The `.env` file should have this variables:

- `AGENTA_API_KEY=xxx`
- `OPENAI_API_KEY=sk-test-xxx`
- `MONGODB_ATLAS_URI=mongodb+srv://xxx:xxx@xxx.xxx.mongodb.net/?retryWrites=true&w=majority&appName=xxx`
- `MONGODB_DATABASE_NAME=xxx`

```python
config = dotenv_values(".env")

openai = OpenAI(api_key=config["OPENAI_API_KEY"])

mongodb = MongoClient(
    config["MONGODB_ATLAS_URI"], tlsCAFile=certifi.where(), server_api=ServerApi("1")
)
db = mongodb[config["MONGODB_DATABASE_NAME"]]
```

The next step is to specify some default configurations for the application using Agenta.
We will set up **[config data type](/reference/sdk/config_datatypes)** such as prompts (`TextParam`), numbers (`FloatParam`, `IntParam`), or dropdowns (`MultipleChoiceParam`).

```python
ag.init()

ag.config.default(
    # RETRIEVER
    retriever_prompt=ag.TextParam("Movies about {topic} in the genre of {genre}."),
    retriever_multiplier=ag.FloatParam(default=3, minval=1, maxval=10),
    # GENERATOR
    generator_context_prompt=ag.TextParam(
        "Given the following list of suggested movies:\n\n{movies}"
    ),
    generator_instructions_prompt=ag.TextParam(
        "Provide a list of {count} movies about {topic} in the genre of {genre}."
    ),
    generator_model=ag.MultipleChoiceParam(
        "gpt-3.5-turbo", ["gpt-4o-mini", "gpt-3.5-turbo"]
    ),
    generator_temperature=ag.FloatParam(default=0.8),
    # SUMMARIZER
    summarizer_context_prompt=ag.TextParam(
        "Act as a professional cinema critic.\nBe concise and factual.\nUse one intro sentence, and one sentence per movie."
    ),
    summarizer_instructions_prompt=ag.TextParam(
        "Summarize the following recommendations about {topic} in the genre of {genre}:\n\n{report}"
    ),
    summarizer_model=ag.MultipleChoiceParam(
        "gpt-4o-mini", ["gpt-4o-mini", "gpt-3.5-turbo"]
    ),
    summarizer_temperature=ag.FloatParam(default=0.2),
)
```

Create a function named `embed` that generates a text embedding for a given description using OpenAI's `text-embedding-ada-002` model.
The function is decorated with `@ag.instrument` to track its execution.

```python
@ag.instrument(
    spankind="EMBEDDING",
    ignore_inputs=["description"],
    ignore_outputs=["embedding", "cost", "usage"],
)
def embed(description: str):
    response = openai.embeddings.create(
        input=description, model="text-embedding-ada-002"
    )
    return {
        "embedding": response.data[0].embedding,
        "cost": ag.calculate_token_usage(
            "text-embedding-ada-002", response.usage.dict()
        ),
        "usage": response.usage.dict(),
    }
```

Let's create a search function that queries the MongoDB collection containing movie embeddings. This funtion will perform a vector
search to find movies similar to the provided query and returns a list of movies with specific attributes (title, genres, plot, and year).

```python
@ag.instrument(spankind="SEARCH", ignore_inputs=True, ignore_outputs=True)
def search(query: list, topk: int):
    embeddings = db["embedded_movies"]

    pipeline = [
        {
            "$vectorSearch": {
                "index": "semantic_similarity_search_index",
                "path": "plot_embedding",
                "queryVector": query,
                "numCandidates": 200,
                "limit": topk,
            }
        },
        {"$project": {"_id": 0, "title": 1, "genres": 1, "plot": 1, "year": 1}},
    ]

    movies = [movie for movie in embeddings.aggregate(pipeline)]

    return movies
```

Define an asynchronous chat function that interacts with the OpenAI API to generate chat completions based on provided prompts.
We can also configure the `model` and `temperature`, while also tracking its execution for performance and cost analysis using Agenta.

```python
@ag.instrument(spankind="MESSAGE")
async def chat(prompts: str, opts: dict):
    response = openai.chat.completions.create(
        model=opts["model"],
        temperature=opts["temperature"],
        messages=[
            {"role": agent, "content": prompt} for (agent, prompt) in prompts.items()
        ],
    )

    return {
        "message": response.choices[0].message.content,
        "cost": ag.calculate_token_usage(opts["model"], response.usage.dict()),
        "usage": response.usage.dict(),
    }
```

Define an asynchronous `retriever` function that generates a prompt based on a `topic` and `genre`, embeds it into a vector, and searches
a database for movies that match the embedded prompt.

```python
@ag.instrument(spankind="RETRIEVER", ignore_inputs=True)
async def retriever(topic: str, genre: str, count: int) -> dict:
    prompt = ag.config.retriever_prompt.format(topic=topic, genre=genre)
    topk = count * ag.config.retriever_multiplier

    ag.tracing.store_internals({"prompt": prompt})

    query = embed(prompt)

    result = search(query["embedding"], topk)

    movies = [
        f"{movie['title']} ({movie['year']}) in {movie['genres']}: {movie['plot']}"
        for movie in result
    ]

    return {"movies": movies}

```

Define an asynchronous function `reporter`, which generates a movie report based on a given `topic`, `genre`, and list of movies.
The function constructs prompts for the language model, sends them to the model, and retrieves a generated report.

```python
@ag.instrument(spankind="GENERATOR", ignore_inputs=True)
async def reporter(topic: str, genre: str, count: int, movies: dict) -> dict:
    context = ag.config.generator_context_prompt.format(movies="\n".join(movies))
    instructions = ag.config.generator_instructions_prompt.format(
        count=count, topic=topic, genre=genre
    )

    prompts = {"system": context, "user": instructions}
    opts = {
        "model": ag.config.generator_model,
        "temperature": ag.config.generator_temperature,
    }

    result = await chat(prompts, opts)

    report = result["message"]

    return {"report": report}
```

Define the asynchronous `summarizer` function that generates a summary based on a given `topic`, `genre`, and `report`.

```python
@ag.instrument(spankind="GENERATOR", ignore_inputs=True)
async def summarizer(topic: str, genre: str, report: dict) -> dict:
    context = ag.config.summarizer_context_prompt
    instructions = ag.config.summarizer_instructions_prompt.format(
        topic=topic, genre=genre, report=report
    )

    prompts = {"system": context, "user": instructions}
    opts = {
        "model": ag.config.summarizer_model,
        "temperature": ag.config.summarizer_temperature,
    }

    result = await chat(prompts, opts)

    report = result["message"]

    return {"report": report}
```

Finally, we will add the `@ag.entrypoint` decorator to the main function of the application. This decorator informs Agenta that this function is the entry point to the application.
It converts it (using FastAPI) into an API endpoint, allowing it to be used from the web interface.

```python
@ag.entrypoint
@ag.instrument(spankind="WORKFLOW")
async def rag(topic: str, genre: str, count: int = 5):
    count = int(count)

    result = await retriever(topic, genre, count)

    result = await reporter(topic, genre, count, result["movies"])

    result = await summarizer(topic, genre, result["report"])

    result = await summarizer(topic, genre, result["report"])

    return result["report"]
```

## Serving the application

To serve the application, we first need to initialize the project in Agenta.
Run the following command in the folder containing the application code and the rest of the files.

```python
agenta init
```

Once this commands runs, you will be prompted to provide the following:

- The name of the application.
- The host for Agenta (**Agenta cloud**) to start from a blank project (yes in this case since we wrote the code) or to populate the folder with a template application (no in this case).

A new new `config.toml` file containing the application’s configuration in the folder as well as a new empty project in the Agenta web UI.

Now, we can serve the application by running the following command.

```python
agenta variant serve app.py
```

The application is now added to the Agenta web interface and can be used from there.

## Evaluation using Agenta

Agenta provides two RAG evaluators for easy evaluation of RAG applications. These includes:

- **RAG Faithfulness**: RAG Faithfulness evaluator assesses the accuracy and reliability of responses generated by RAG models.
  It evaluates how faithfully the responses adhere to the retrieved documents or sources, ensuring that the generated text accurately
  reflects the information from the original sources.

- **RAG Context Relevancy**: RAG Context Relevancy evaluator measures how relevant the retrieved documents or contexts are to the given question or prompt.
  It ensures that the selected documents provide the necessary information for generating accurate and meaningful responses, improving the overall quality of the RAG model's output.

After serving the RAG Application with Agenta, you should see the output of the second summarizer. Upon clicking on `view trace`, you should be able to see the `inputs` and `outputs` for each instrumented stage in the RAG workflow.

<img
  height="600"
  className="dark:hidden"
  src="/images/basic_guides/rag_initial_trace_light.png"
/>
<img
  height="600"
  className="hidden dark:block"
  src="/images/basic_guides/rag_initial_trace_dark.png"
/>

On the side bar, click on **Automatic Evaluation** and select **Evaluators**. Click on the **New Evaluator** button and you will see a modal, select **RAG Faithfulness** from the list of evaluators.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-RAG-select.png"
/>

Configure the **RAG Context Relevancy** evaluator by providing the following information:

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-RAG-configuration.png"
/>

- `Name`: A unique name for your evaluator.
- `Question Key`: The input question to the LLM application. This is the question used to retrieve the context and formulate the answer.
- `Answer Key`: The output answer generated by the LLM application. This should point to the answer formulated based on the input question and the retrieved context.
- `Contexts Key`: The documents or snippets retrieved by the LLM application in the RAG workflow. These contexts are used to assess the faithfulness of the generated answer.

RAG evaluators, based on [RAGAS](https://docs.ragas.io/) are different from other evaluators in Agenta in that they often require internal variables. For instance, [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html) and [Context Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html) both require `question`, `answer`, and `contexts`.

From the trace we saw before, we could say that `answer` maps to the second rag summarizer output report, denoted by `rag.summarizer[1].outputs.report`. Similarly, the `contexts` map to the rag retriever output movies, denoted by `rag.retriever.outputs.movies`.

However, the `question`, which corresponds to the formatted prompt sent to the retriever (or the reporter) is not part of the inputs or the outputs of any stage in the workflow.

Next, let's see how and where you to find that variable.

In Agenta, when instrumenting stages of a workflow, there is a utility to add internal variables, called `internals`, to the stage's span.

```python
async def retriever(topic: str, genre: str, count: int) -> dict:
    prompt = ag.config.retriever_prompt.format(topic=topic, genre=genre)
    topk = count * ag.config.retriever_multiplier

    ag.tracing.store_internals({"prompt": prompt})

    query = embed(prompt)
    result = search(query["embedding"], topk)

    movies = [
        f"{movie['title']} ({movie['year']}) in {movie['genres']}: {movie['plot']}"
        for movie in result
    ]

    return {"movies": movies}
```

The utility `ag.tracing.store_internals(...)`, as shown in the snippet above.

Upon serving the RAG Application again, the trace should now contain the formatted prompt

<img
  height="600"
  className="dark:hidden"
  src="/images/basic_guides/rag_final_trace_light.png"
/>
<img
  height="600"
  className="hidden dark:block"
  src="/images/basic_guides/rag_final_trace_dark.png"
/>

We are ready to resume the configuration of our RAG Evaluator.

Back to the evaluators page, we can now map the `question` to the rag retriever internal prompt, denoted by `rag.retriever.internals.prompt`, as shown below.

<img
  height="600"
  className="dark:hidden"
  src="/images/basic_guides/rag_final_faithfulness_light.png"
/>
<img
  height="600"
  className="hidden dark:block"
  src="/images/basic_guides/rag_final_faithfulness_dark.png"
/>

Once the RAG Evaluator is set up, you are ready to go.

## Running the evaluation

You can now run evaluations on RAG Applications based on **RAG Evaluators**.

<img
  height="600"
  className="dark:hidden"
  src="/images/basic_guides/rag_evaluation_light.png"
/>
<img
  height="600"
  className="hidden dark:block"
  src="/images/basic_guides/rag_evaluation_dark.png"
/>
