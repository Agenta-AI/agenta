---
title: "Pattern Matching Evaluation"
---

The term **LLM-as-a-Judge** is an evaluation method where a Large Language Model is used to assess the quality of the model's generated content or output.
This method automates the evaluation process, making it quicker by comparing the output from different versions of a model and decides which one is better based on predefined criteria.

In Agenta, you can make use of the **AI critique evaluator** to perform this evaluation.

The **AI critique evaluator** sends the generated answer and the `correct_answer` to an LLM model and uses it to evaluate the correctness of the answer.
All you need is to provide the evaluation prompt (or use the default prompt).

## AI Critique Evaluator

On the sidebar, you will see **Automatic Evaluation**, select **Evaluators** and click on the **New Evaluator** button.
From the pop up, you will have to select the **AI critique evaluator**.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-AI-Critique-select.png"
/>

Proceed to give the evaluator a unique name. A prompt template is added for AI critique prompts by default in Agenta which can be modified to suit your use case.
But in this guide, we will use the default prompt template.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-AI-Critique-evaluator-configuration.png"
/>

In the advanced settings, you add the name of the column in the test data that contains the correct answer. Click **Save**.

In the Results section, create a new evaluation using the **New Evaluation** button and fill in the required data, such as the test set you want to use? the variants you would like to evaluate, and
which evaluators would you like to evaluate (in this case LLM-as-a-Judge)?

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-AI-Critique-result-config.png"
/>

For advanced configuration, toggle on the Advanced configuration button and set the following:

- Batch size ( Number of testset to have in each batch )
- Max retries ( Maximum number of times to retry the failed llm call )
- Retry delay ( Delay before retrying the failed llm call (in seconds) )
- Delay between batches ( Delay to run batches (in seconds) )
- Correct answer column ( Column in the test set containing the correct/expected answer )

## View Evaluation Result

The latest evaluation is displayed in the Results section with a `completed` status.
You can click on the evaluation to get a full detail such as the `input`, `ground_truth`, `output`, `LLM-as-a-judge` score, `cost` and `latency`.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-AI-Critique-Evaluation-result.png"
/>

## Compare Evaluations

Agenta also allows you to compare two or more evaluations from the same testset.
All you have to do is select the two testsets (the checkboxes on the left) click on the **Compare** button, and you will see a table displaying the output of the evaluation comparison.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-AI-Critique-Evaluation-Comparion-result.png"
/>
