---
title: "Semantic Analysis Evaluation"
---

Semantic Analysis Evaluation involves assessing the meaning and intent behind the mode'ls generated output, rather than just focusing on exact word matches.
The goal is to determine whether the output conveys the intended meaning accurately, even if the wording is different from the correct answer.

In Agenta there are three ways to perform semantic analysis evaluation using the following evaluators:

- Similarity Match
- Semantic Similarity Match
- Levenshtein Distance

## Similarity Match

The **Similarity Match evaluator** is used to determine how closely the generated answer matches the expected answer.
Instead of requiring an exact match, it checks for similarity between the two texts.

### Key Features:

- **Similarity Threshold**: You will need to set a similarity threshold, which is a value that determines how close the two answers need to be in order to be considered a match.
- **Jaccard Similarity**: This evaluator uses the Jaccard similarity method to compare the answers. Jaccard similarity measures the similarity between two sets of data by dividing the number of common elements by the total number of unique elements.
  In this context, it compares the sets of words in the generated and expected answers to see how much they overlap.

To make use of this evaluator, click on the **Automatic Evaluation** section in the sidebar and select **Evaluators**, click on the **New Evaluator** button.
Select **Similarity Match** from the list of evaluators.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Similarity-match-select.png"
/>

Configure the Similarity Match evaluator by adding a unique name in the `Name` field, and setting the `Similarity Threshold` to your desired value.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Similarity-match-configuration.png"
/>

Click **Save**.

Click on **Results** and in the top right corner, click on the **New Evaluation** button. To create a new eveluation you need to select which testset do you want to use, wich variants you would like to evaluate, and which evaluator you would like to evaluate on.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Similarity-match-new-evaluation.png"
/>

<Note>
  {" "}
  For advanced configuration, toggle on the Advanced configuration button and set the following:

- Batch size ( Number of testset to have in each batch )
- Max retries ( Maximum number of times to retry the failed llm call )
- Retry delay ( Delay before retrying the failed llm call (in seconds) )
- Delay between batches ( Delay to run batches (in seconds) )
- Correct answer column ( Column in the test set containing the correct/expected answer ) {" "}

</Note>

The evaluation runs and once it is complete you will be able to see the evaluation results.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Similarity-match-evaluation-result.png"
/>

## Semantic Similarity Match

**Semantic Similarity Match** evaluator measures the similarity between two pieces of text by analyzing their meaning and context.
It compares the semantic content, providing a score that reflects how closely the texts match in terms of meaning, rather than just exact word matches.

To begin, let's select the Semantic Similarity Match evaluator from the list of evaluators.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Semantic-Similarity-match-select.png"
/>

Configuring the Semantic Similarity Match evaluator is very easy, the only thing you need to do is give it a unique `name`, unlike the **Similarity match** evaluator where you also need to add `Similarity Threshold`

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Semantic-Similarity-configuration.png"
/>

Click **Save**.

Proceed to the **Results** section where you choose your variants and evaluator to start the evaluation process.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Semantic-Similarity-new-evaluation.png"
/>

Click on the just completed evaluation and you should be be able to see the evaluation result.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Semantic-Similarity-evaluation-result.png"
/>

## Levenshtein Distance

This evaluator calculates the Levenshtein distance between the output and the correct answer.
If a threshold is provided in the settings, it returns a boolean indicating whether the distance is within the threshold.
If no threshold is provided, it returns the actual Levenshtein distance as a numerical value.

To evaluate using the **Levenshtein Distance** evaluator, in the **Automatic Evaluation** section click on the **New Evaluator** button.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Levenshtein-Distance-select.png"
/>

To configure the Levenshtein Distance evaluator, add a unique `name` and a `threshold` (The maximum allowed Levenshtein distance between the output and the correct answer).

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Levenshtein-Distance-configuration.png"
/>

We will not add a threshold in this guide. Click **Save**.

<Note>
  {" "}
  If a threshold is provided in the settings, it returns a boolean indicating whether
  the distance is within the threshold. If no threshold is provided, it returns the
  actual Levenshtein distance as a numerical value.{" "}
</Note>

Create a New Evaluation, Choose your testset, variant and evaluator (Levenshtein Distance evaluator) to start the evaluation process.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Levenshtein-Distance-new-evaluation.png"
/>

The evaluator runs the evaluation process on the given variant and testset, and when it is complete you can view the evaluation result.

<img
  className="dark:hidden"
  height="600"
  src="/images/Evaluators/Agenta-Levenshtein-Distance-evaluation-result.png"
/>

From the evaluation result you will see that 0, 6, 7, and 44 were Levenshtein distances calculated between the output generated by your model and the correct answers in the testset.
Here's what each value signifies:

- 0: The output matches the correct answer exactly. This is the best possible score, indicating a perfect match.

- 6: The output differs from the correct answer by 6 single-character edits. This means that to turn the output into the correct answer, you would need to make 6 changes (insertions, deletions, or substitutions).

- 7: Similar to the previous case, the output requires 7 edits to match the correct answer. This indicates that the output is slightly more different than in the case with a distance of 6.

- 44: This indicates a significant difference between the output and the correct answer, requiring 44 edits. The larger the number, the less similar the output is to the correct answer.
