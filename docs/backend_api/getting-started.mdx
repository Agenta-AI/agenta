---
title: Getting Started
description: 'Building your first Large Language Model Application in Agenta'
---

## Introduction

Welcome! In this beginner-friendly guide, we'll walk through the process of building a simple LLM app in Agenta, using an existing template. By the end of this tutorial, you'll have your first working LLM app and a good understanding of how to interact with Agenta.

To learn more about creating an LLM app from scratch, please visit our [advanced tutorial](/tutorials/your-first-llm-app).

## Prerequisites

Ensure you have installed the Agenta CLI and SDK, and have set up the web platform using docker-compose. If you haven't done this, follow the steps in our [installation guide](/installation).

## Step 1: Initialize a New Project

To start, create a new project. Begin by making an empty folder and initialize it using the following command:

```bash
mkdir example_app; cd example_app
agenta init
```

Next, start a new project based on the `simple_prompt` [template](https://docs.agenta.ai/conceptual/concepts#templates)

````
? Please enter the app name: company_name_generator
? How do you want to initialize your app? Start from template
? Which template do you want to use? simple_prompt - Simple app that uses one prompt using langchain
App initialized successfully
````

This will generate many files for this project. The file containig the code is `app.py`. You can check a description of the generated code. Otherwise, continue to the next step.


<Accordion title="Understand the Generated Code">

The initialization process generates the following files:

```bash
.
├── README.md
├── app.py
├── config.toml
└── requirements.txt
```

Let's take a closer look at each:

### README.md
This file provides a brief overview of your application.

### requirements.txt
Here you'll find the dependencies required by the template.

### config.toml
This file holds the configuration details of your application. Currently, it only contains the app name that you have chosen.

### app.py
This is the main application file. The generated `app.py` file should look like this:

```python app.py
# imports
import agenta as ag

default_prompt = "What is a good name for a company that makes {product}?"


@ag.post
def generate(product: str, temperature: ag.FloatParam = 0.9, prompt_template: ag.TextParam = default_prompt) -> str:
    llm = OpenAI(temperature=temperature)
    prompt = PromptTemplate(
        input_variables=["product"],
        template=prompt_template,
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    output = chain.run(product=product)

    return output
```

This code sets up a simple LLM-application using the OpenAI Python library. The decorator `@post` and the types `FloatParam` and `TextParam` are imported from `agenta`, which inform the Agenta SDK that this function is an endpoint of the application, and these parameters can be adjusted in the playground.

</Accordion>

## Step 2: Set up the OpenAI API Key

To run your LLM-application, you'll need to set the OpenAI API key in your environment. Here's how:

1. Visit the [OpenAI API keys page](https://platform.openai.com/account/api-keys), and create a new key. 

2. Create a file `.env` in your project folder and add the line:
```bash
OPENAI_API_KEY=sk-xxxxxxx
```

## Step 3: Run the Application in the CLI

Test your application via the command line.

First let's (optionally) create a virtual environment and install the dependencies. Make sure you are in the project folder.

```bash
python -m venv myenv
source myenv/bin/activate
pip install -r requirements.txt
```

Now let's run the application:

```bash
> python app.py
usage: app.py [-h] [--temperature TEMPERATURE] [--prompt_template PROMPT_TEMPLATE] product
app.py: error: the following arguments are required: product

> python app.py "LLMOps developer platform"

CypressOps Platform.
```

Here, `app.py` uses the `product` as the inputs defined in the parameters in `app.py`.

## Step 5: Run the Application in the Playground

To run the application in the playground, you need to serve it locally first. Use the command `agenta variant serve` to do this. This command:

1. Creates a Docker image of the application, and starts a container based on this image.
2. Starts a server that exposes an endpoint /generate at `localhost/{app_name}/{variant_name}/openapi.json` with the parameters specified in the application.
3. Adds the application to the Agenta's web UI.

```bash
agenta variant serve --file_name app.py
```

This will do two things:
1. It will create a docker image of the application, and start a container based on this image.
2. The container will start a server exposing an endpoint /generate with the same parameters we specified in the application at `localhost/{app_name}/{variant_name}/openapi.json`.
3. It will add the application to the web ui Agenta

## Step 6: Start Experimenting

Go to `localhost:3000`, select your app, and begin experimenting with different parameters in the playground.

<Frame>
  <img
    className="rounded-md"
    src="docs/images/playground.png"
  />
</Frame>

The playground provides a platform to create new app variants from the one you just made, and to tweak the application parameters. 

We hope you found this guide useful. Happy coding!


