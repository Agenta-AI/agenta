---
title: "Observability and Prompt Management"
slug: observability-and-prompt-management
date: 2024-11-06
tags: [v0.27.0]
---


import Image from "@theme/IdealImage";




<Image
  style={{
    display: "block",
    margin: "20px auto",
    textAlign: "center",
  }}
  img={require("/images/observability/observability.png")}
  alt="Observability view showing an open trace for an OpenAI application"
  loading="lazy"
/>
This release is one of our biggest yet—one changelog hardly does it justice.

**First up: Observability**

We’ve had observability in beta for a while, but now it’s been completely rewritten,
with a brand-new UI and fully **open-source code**.

The new [Observability SDK](/observability/overview) is compatible with [OpenTelemetry (Otel)](https://opentelemetry.io/) and [gen-ai semantic conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/). This means you get a lot of integrations right out of the box, like [LangChain](/observability/integrations/langchain), [OpenAI](/observability/integrations/openai), and more.

We’ll publish a full blog post soon, but here’s a quick look at what the new observability offers:

- A redesigned UI that lets you visualize nested traces, making it easier to understand what’s happening behind the scenes.

- The web UI lets you filter traces by name, cost, and other attributes—you can even search through them easily.

- The SDK is Otel-compatible, and we’ve already tested integrations for [OpenAI](/observability/integrations/openai), [LangChain](/observability/integrations/langchain), [LiteLLM](/observability/integrations/litellm), and [Instructor](/observability/integrations/instructor), with guides available for each. In most cases, adding a few lines of code will have you seeing traces directly in Agenta.

**Next: Prompt Management**

We’ve completely rewritten the [prompt management SDK](/prompt-engineering/managing-prompts-programatically/setup), giving you full CRUD capabilities for prompts and configurations. This includes creating, updating, reading history, deploying new versions, and deleting old ones. You can find a first tutorial for this [here](/tutorials/sdk/manage-prompts-with-SDK).

**And finally: LLM-as-a-Judge Overhaul**

We've made significant upgrades to the [LLM-as-a-Judge evaluator](/evaluation/configure-evaluators/llm-as-a-judge). It now supports prompts with multiple messages and has access to all variables in a test case. You can also switch models (currently supporting OpenAI and Anthropic). These changes make the evaluator much more flexible, and we're seeing better results with it.

<Image
  style={{
    display: "block",
    margin: "5px auto",
    width: "50%",
    textAlign: "center",
  }}
  img={require("/images/evaluation/llm-as-a-judge.gif")}
  alt="Configuring the LLM-as-a-Judge evaluator"
  loading="lazy"
/>

---
