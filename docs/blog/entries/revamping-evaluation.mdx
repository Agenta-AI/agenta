---
title: "Revamping evaluation"
slug: revamping-evaluation
date: 2024-01-22
tags: [v0.8.0]
---


import Image from "@theme/IdealImage";




We've spent the past month re-engineering our evaluation workflow. Here's what's new:

**Running Evaluations**

1. Simultaneous Evaluations: You can now run multiple evaluations for different app variants and evaluators concurrently.

<Image img={require("/images/changelog/eval_1.png")} />

2. Rate Limit Parameters: Specify these during evaluations and reattempts to ensure reliable results without exceeding open AI rate limits.

<Image img={require("/images/changelog/eval_2.png")} />

3. Reusable Evaluators: Configure evaluators such as similarity match, regex match, or AI critique and use them across multiple evaluations.

<Image img={require("/images/changelog/eval_3.png")} />

**Evaluation Reports**

1. Dashboard Improvements: We've upgraded our dashboard interface to better display evaluation results. You can now filter and sort results by evaluator, test set, and outcomes.

<Image img={require("/images/changelog/eval_4.png")} />

2. Comparative Analysis: Select multiple evaluation runs and view the results of various LLM applications side-by-side.

<Image img={require("/images/changelog/eval_5.png")} />

---
