from copy import deepcopy


rag_evaluator_settings_template = {
    "question_key": {
        "label": "Question Key",
        "default": "",
        "type": "string",
        "required": True,
        "advanced": False,
        "description": "The input question to the LLM application. This is the question used to retrieve the context and formulate the answer.",
    },
    "answer_key": {
        "label": "Answer Key",
        "default": "",
        "type": "string",
        "required": True,
        "advanced": False,
        "description": "The output answer generated by the LLM application. This should point to the answer formulated based on the input question and the retrieved context.",
    },
    "contexts_key": {
        "label": "Contexts Key",
        "default": "",
        "type": "string",
        "required": True,
        "advanced": False,
        "description": "The documents or snippets retrieved by the LLM application in the RAG workflow. These contexts are used to assess the faithfulness of the generated answer.",
    },
}
evaluators = [
    {
        "name": "LLM-as-a-judge",
        "key": "auto_ai_critique",
        "direct_use": False,
        "requires_llm_api_keys": True,
        "settings_presets": [
            {
                "key": "hallucination",
                "name": "Hallucination Detection",
                "values": {
                    "prompt_template": [
                        {
                            "role": "system",
                            "content": "You are an expert evaluator grading model outputs for hallucinations. Your task is to identify if the responses contain any hallucinated information based on the criteria and requirements provided below. \n\nGiven the model output and inputs (and any other data you might get) determine if the output contains hallucinations. \n\n## Hallucination considerations\n- Verify all factual claims in the output meticulously against the input data\n- Identify any information that is fabricated or not supported by the input data\n- Differentiate between minor inaccuracies and major hallucinations\n\n## Output format\nANSWER ONLY 'true' IF THE OUTPUT CONTAINS HALLUCINATIONS, OTHERWISE ANSWER 'false'. DO NOT USE MARKDOWN. DO NOT PROVIDE ANYTHING OTHER THAN 'true' OR 'false'\n",
                        },
                        {
                            "role": "user",
                            "content": "## Model inputs\n{{inputs}}\n## Model outputs\n{{outputs}}",
                        },
                    ],
                    "model": "gpt-4o-mini",
                    "response_type": "json_schema",
                    "json_schema": {
                        "name": "schema",
                        "schema": {
                            "title": "extract",
                            "description": "Extract information from the user's response.",
                            "type": "object",
                            "properties": {
                                "score": {
                                    "type": "boolean",
                                    "description": "The hallucination detection result",
                                }
                            },
                            "required": [
                                "score",
                            ],
                            "additionalProperties": False,
                        },
                        "strict": True,
                    },
                    "version": "4",
                },
            },
            {
                "key": "conciseness",
                "name": "Conciseness",
                "values": {
                    "prompt_template": [
                        {
                            "role": "system",
                            "content": 'You are an expert data labeler evaluating AI-generated responses for conciseness. Your task is to assess how efficiently the response delivers the requested information without unnecessary content.\n\n## Task\nEvaluate the response\'s conciseness on a scale from 0 to 1.\n\n## Scoring Scale\n- **1.0**: Maximally concise. Contains only essential information in minimal words.\n- **0.7-0.9**: Mostly concise with minor excess (one or two unnecessary phrases).\n- **0.4-0.6**: Moderately verbose. Contains noticeable filler but core answer is present.\n- **0.1-0.3**: Significantly verbose. Buried answer with substantial unnecessary content.\n- **0.0**: Entirely verbose or non-responsive.\n\n## What Constitutes Perfect Conciseness\nA perfectly concise response:\n- Delivers exactly what was asked—nothing more\n- Uses the fewest words possible while remaining complete\n- Omits pleasantries, hedging, and filler\n- Excludes self-referential commentary ("I think...", "As an AI...")\n- Avoids unrequested explanations or context\n- Contains no redundancy or restatement\n\n## Deduction Triggers\nPenalize responses containing:\n- Opening phrases: "Sure!", "Great question!", "I\'d be happy to help"\n- Hedging: "probably", "I believe", "as far as I know"\n- Self-reference: "I think", "In my opinion", "As an AI"\n- Unrequested context or background\n- Closing phrases: "Hope this helps!", "Let me know if you need more"\n- Follow-up offers: "Would you like me to explain further?"\n- Redundant restatements of the same point',
                        },
                        {
                            "role": "user",
                            "content": "## Model inputs\n{{inputs}}\n## Model outputs\n{{outputs}}",
                        },
                    ],
                    "model": "gpt-4o-mini",
                    "response_type": "json_schema",
                    "json_schema": {
                        "name": "schema",
                        "schema": {
                            "title": "extract",
                            "description": "Extract information from the user's response.",
                            "type": "object",
                            "properties": {
                                "score": {
                                    "type": "number",
                                    "description": "The grade results",
                                    "minimum": 0,
                                    "maximum": 1,
                                },
                                "comment": {
                                    "type": "string",
                                    "description": "Reasoning for the score",
                                },
                            },
                            "required": [
                                "score",
                                "comment",
                            ],
                            "additionalProperties": False,
                        },
                        "strict": True,
                    },
                    "version": "4",
                },
            },
            {
                "key": "answer_relevancy",
                "name": "Answer Relevancy",
                "values": {
                    "prompt_template": [
                        {
                            "role": "system",
                            "content": 'You are an expert data labeler evaluating AI-generated responses for answer relevancy. Your task involves two steps: generating a question the response would answer, then assessing whether the response is noncommittal.\n\n## Task\nGiven a response, perform two evaluations:\n1. Generate the most likely question this response is attempting to answer\n2. Determine if the response is noncommittal (evasive/vague) or committal (direct/specific)\n\n\n## Step 1: Question Generation\nInfer the question the response is most likely answering. The question should:\n- Be specific and direct\n- Match the subject matter and scope of the response\n- Represent what a user would realistically ask to receive this response\n\n## Step 2: Noncommittal Assessment\nEvaluate whether the response directly answers the inferred question or evades it.\n\n**Score 1 (Committal - Good)**: The response provides a direct, specific answer. Indicators include:\n- States concrete facts, dates, names, or figures\n- Takes a clear position or provides definitive information\n- Answers the question without hedging or deflecting\n\n**Score 0 (Noncommittal - Bad)**: The response avoids giving a direct answer. Indicators include:\n- Explicit uncertainty: "I don\'t know", "I\'m not sure", "I cannot answer"\n- Knowledge cutoff disclaimers: "I\'m unaware of information beyond..."\n- Deflection: "You should ask someone else", "That depends on many factors"\n- Vague generalizations instead of specific answers\n- Refusal to answer: "I can\'t help with that"\n\n## Examples\n\n**Example 1**\nResponse: "Albert Einstein was born in Germany."\n\nOutput:\n{\n  "question": "Where was Albert Einstein born?",\n  "score": 1\n}\n\n**Example 2**\nResponse: "I don\'t know about the groundbreaking feature of the smartphone invented in 2023 as I am unaware of information beyond 2022."\n\nOutput:\n{\n  "question": "What was the groundbreaking feature of the smartphone invented in 2023?",\n  "score": 0\n}\n\n**Example 3**\nResponse: "The capital of France is Paris, which has been the capital since the 10th century."\n\nOutput:\n{\n  "question": "What is the capital of France?",\n  "score": 1\n}\n\n**Example 4**\nResponse: "That\'s a complex topic with many perspectives. It really depends on who you ask and what criteria you use."\n\nOutput:\n{\n  "question": "What is the best programming language for beginners?",\n  "score": 0\n}',
                        },
                        {
                            "role": "user",
                            "content": "## Model outputs\n{{outputs}}",
                        },
                    ],
                    "model": "gpt-4o-mini",
                    "response_type": "json_schema",
                    "json_schema": {
                        "name": "schema",
                        "schema": {
                            "title": "extract",
                            "description": "Extract information from the user's response.",
                            "type": "object",
                            "properties": {
                                "question": {
                                    "type": "string",
                                    "description": "The inferred question the response answers",
                                },
                                "score": {
                                    "type": "number",
                                    "description": "Score where 1 = committal (direct/specific - good), 0 = noncommittal (evasive/vague - bad)",
                                    "enum": [0, 1],
                                },
                            },
                            "required": [
                                "question",
                                "score",
                            ],
                            "additionalProperties": False,
                        },
                        "strict": True,
                    },
                    "version": "4",
                },
            },
            {
                "key": "helpfulness",
                "name": "Helpfulness",
                "values": {
                    "prompt_template": [
                        {
                            "role": "system",
                            "content": 'You are an expert data labeler evaluating AI-generated responses for helpfulness. Your task is to assess how effectively the response addresses the user\'s needs and assists them in achieving their goal.\n\n## Task\nEvaluate the response\'s helpfulness on a scale from 0 to 1.\n\n\n## Scoring Scale\n- **1.0**: Fully helpful. Accurately addresses the query, provides actionable information, and is clear and engaging.\n- **0.7-0.9**: Mostly helpful. Addresses the core query with minor gaps in clarity, completeness, or tone.\n- **0.4-0.6**: Partially helpful. Provides some useful information but misses key aspects or lacks clarity.\n- **0.1-0.3**: Minimally helpful. Touches on the topic but fails to meaningfully assist the user.\n- **0.0**: Not helpful. Incorrect, irrelevant, or refuses to engage with the query.\n\n## What Constitutes a Helpful Response\nA fully helpful response:\n- Directly addresses the user\'s actual question or need\n- Provides accurate and factually correct information\n- Includes relevant details that aid understanding or resolution\n- Is clear, well-structured, and easy to follow\n- Offers actionable guidance when the query calls for it\n- Maintains an appropriate and engaging tone\n- Anticipates reasonable follow-up needs without over-explaining\n\n## Evaluation Criteria\n\n**Accuracy** (Is the information correct?)\n- Deduct heavily for factual errors or misleading information\n- Deduct for outdated information when current data is available\n\n**Relevance** (Does it address what was asked?)\n- Deduct for tangential information that doesn\'t serve the query\n- Deduct for answering a different question than what was asked\n\n**Completeness** (Does it cover the necessary ground?)\n- Deduct for missing critical information needed to resolve the query\n- Deduct for superficial answers when depth is required\n\n**Clarity** (Is it easy to understand?)\n- Deduct for confusing explanations or poor organization\n- Deduct for unnecessary jargon without explanation\n\n**Actionability** (Can the user act on it?)\n- Deduct for vague suggestions when specific steps are needed\n- Deduct for theoretical answers to practical questions\n\n## Output Format\nReturn a JSON object:\n{\n  "score": <float between 0 and 1>,\n  "reasoning": "<brief explanation of score>"\n}\n\n## Examples\n\n**Example 1**\nQuestion: "How do I reset my password on Gmail?"\nResponse: "Go to the Gmail sign-in page, click \'Forgot password\', enter your email, and follow the verification steps. You\'ll receive a code via SMS or backup email to create a new password."\n\nOutput:\n{\n  "score": 1.0,\n  "reasoning": "Provides accurate, step-by-step instructions that directly address the query. Clear, actionable, and complete."\n}\n\n**Example 2**\nQuestion: "What\'s the best way to learn Python?"\nResponse: "Python is a programming language created by Guido van Rossum in 1991. It\'s used for web development, data science, and automation."\n\nOutput:\n{\n  "score": 0.2,\n  "reasoning": "Provides background information about Python but completely ignores the actual question about how to learn it. Not actionable."\n}\n\n**Example 3**\nQuestion: "Why is my plant\'s leaves turning yellow?"\nResponse: "Yellow leaves can indicate overwatering, underwatering, nutrient deficiency, or insufficient light. Check if the soil is soggy (overwatering) or bone dry (underwatering). If watering seems fine, consider a balanced fertilizer or moving the plant to brighter indirect light."\n\nOutput:\n{\n  "score": 0.9,\n  "reasoning": "Covers multiple causes with diagnostic steps and solutions. Minor deduction for not asking clarifying questions about the plant type, which could affect advice."\n}',
                        },
                        {
                            "role": "user",
                            "content": "## Model inputs\n{{inputs}}\n## Model outputs\n{{outputs}}",
                        },
                    ],
                    "model": "gpt-4o-mini",
                    "response_type": "json_schema",
                    "json_schema": {
                        "name": "schema",
                        "schema": {
                            "title": "extract",
                            "description": "Extract information from the user's response.",
                            "type": "object",
                            "properties": {
                                "score": {
                                    "type": "number",
                                    "description": "Helpfulness score from 0 to 1",
                                    "minimum": 0,
                                    "maximum": 1,
                                },
                                "reasoning": {
                                    "type": "string",
                                    "description": "Brief explanation of the score",
                                },
                            },
                            "required": [
                                "score",
                                "reasoning",
                            ],
                            "additionalProperties": False,
                        },
                        "strict": True,
                    },
                    "version": "4",
                },
            },
        ],
        "settings_template": {
            "prompt_template": {
                "label": "Prompt Template",
                "type": "messages",
                "description": "Template for AI critique prompts",
                "required": True,
                "default": [
                    {
                        "role": "system",
                        "content": "You are an expert evaluator grading model outputs. Your task is to grade the responses based on the criteria and requirements provided below. \n\nGiven the model output and inputs (and any other data you might get) assign a grade to the output. \n\n## Grading considerations\n- Evaluate the overall value provided in the model output\n- Verify all claims in the output meticulously\n- Differentiate between minor errors and major errors\n- Evaluate the outputs based on the inputs and whether they follow the instruction in the inputs if any\n- Give the highst and lowest score for cases where you have complete certainty about correctness and value\n\n## output format\nDO NOT USE MARKDOWN. \n",
                    },
                    {
                        "role": "user",
                        "content": "## Model inputs\n{{inputs}}\n## Model outputs\n{{outputs}}",
                    },
                ],
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "required": False,
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
            "model": {
                "label": "Model",
                "default": "gpt-4o",
                "type": "multiple_choice",
                "options": [
                    "gpt-4-turbo",
                    "gpt-4o",
                    "gpt-4o-mini",
                    "gpt-5",
                    "gpt-5-mini",
                    "gpt-5-nano",
                    "claude-3-7-sonnet-20250219",
                    "claude-opus-4-20250514",
                    "claude-sonnet-4-20250514",
                    "claude-opus-4-1-20250805",
                    "claude-sonnet-4-5-20250929",
                ],
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "description": "The LLM model to use for the evaluation",
            },
            "response_type": {
                "label": "Response Type",
                "default": "json_schema",
                "type": "hidden",
                "advanced": True,
                "description": "The format of the response from the LLM",
            },
            "json_schema": {
                "label": "Feedback Configuration",
                "default": {
                    "name": "schema",
                    "schema": {
                        "title": "extract",
                        "description": "Extract information from the user's response.",
                        "type": "object",
                        "properties": {
                            "score": {
                                "type": "boolean",
                                "description": "The grade results",
                            }
                        },
                        "required": [
                            "score",
                        ],
                        "additionalProperties": False,
                    },
                    "strict": True,
                },
                "type": "llm_response_schema",
                "advanced": False,
                "description": "Select a response format to structure how your evaluation results are returned.",
            },
            "version": {
                "label": "Version",
                "type": "hidden",
                "default": "4",
                "description": "The version of the evaluator",  # ignore by the FE
                "advanced": False,  # ignore by the FE
            },
        },
        "description": "LLM-as-a-judge uses a configurable prompt template that takes the output—and optionally inputs or data from the testcase such as correct answer—to evaluate the generated output.",
        "oss": True,
        "tags": ["ai_llm"],
    },
    {
        "name": "Code Evaluation",
        "key": "auto_custom_code_run",
        "direct_use": False,
        "settings_presets": [
            {
                "key": "python_default",
                "name": "Exact Match (Python)",
                "values": {
                    "requires_llm_api_keys": False,
                    "runtime": "python",
                    "correct_answer_key": "correct_answer",
                    "code": "from typing import Dict, Union, Any\n\n\ndef evaluate(\n    app_params: Dict[str, str],  # deprecated; currently receives {}\n    inputs: Dict[str, str],\n    output: Union[str, Dict[str, Any]],\n    correct_answer: str,\n) -> float:\n    if output == correct_answer:\n        return 1.0\n    return 0.0\n",
                },
                "description": "Exact match evaluator implemented in Python.",
            },
            {
                "key": "javascript_default",
                "name": "Exact Match (JavaScript)",
                "values": {
                    "requires_llm_api_keys": False,
                    "runtime": "javascript",
                    "correct_answer_key": "correct_answer",
                    "code": 'function evaluate(appParams, inputs, output, correctAnswer) {\n  void appParams\n  void inputs\n\n  const outputStr =\n    typeof output === "string" ? output : JSON.stringify(output)\n\n  return outputStr === String(correctAnswer) ? 1.0 : 0.0\n}\n',
                },
                "description": "Exact match evaluator implemented in JavaScript.",
            },
            {
                "key": "typescript_default",
                "name": "Exact Match (TypeScript)",
                "values": {
                    "requires_llm_api_keys": False,
                    "runtime": "typescript",
                    "correct_answer_key": "correct_answer",
                    "code": 'type OutputValue = string | Record<string, unknown>\n\nfunction evaluate(\n  app_params: Record<string, string>,\n  inputs: Record<string, string>,\n  output: OutputValue,\n  correct_answer: string\n): number {\n  void app_params\n  void inputs\n\n  const outputStr =\n    (typeof output === "string" ? output : JSON.stringify(output)) as string\n\n  return outputStr === String(correct_answer) ? 1.0 : 0.0\n}\n',
                },
                "description": "Exact match evaluator implemented in TypeScript.",
            },
        ],
        "settings_template": {
            "requires_llm_api_keys": {
                "label": "Requires LLM API Key(s)",
                "type": "boolean",
                "required": True,
                "default": False,
                "advanced": True,
                "description": "Indicates whether the evaluation requires LLM API key(s) to function.",
            },
            "code": {
                "label": "Evaluation Code",
                "type": "code",
                "default": "from typing import Dict, Union, Any\n\n\ndef evaluate(\n    app_params: Dict[str, str],  # deprecated; currently receives {}\n    inputs: Dict[str, str],\n    output: Union[str, Dict[str, Any]],\n    correct_answer: str,\n) -> float:\n    if output == correct_answer:\n        return 1.0\n    return 0.0\n",
                "description": "Code for evaluating submissions",
                "required": True,
            },
            "runtime": {
                "label": "Runtime",
                "type": "multiple_choice",
                "default": "python",
                "options": ["python", "javascript", "typescript"],
                "advanced": True,
                "description": "Runtime environment used to execute the evaluator code.",
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "required": False,
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer. This will be shown in the results page.",
            },
        },
        "description": "Code Evaluation allows you to write your own evaluator in Python. You need to provide the Python code for the evaluator.",
        "oss": True,
        "tags": ["custom"],
    },
    {
        "name": "JSON Field Match",
        "key": "field_match_test",
        "direct_use": False,
        "archived": True,  # Deprecated - use json_multi_field_match instead
        "settings_template": {
            "json_field": {
                "label": "JSON Field",
                "type": "string",
                "default": "",
                "description": "The name of the field in the JSON output that you wish to evaluate",
                "required": True,
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "description": "JSON Field Match evaluator compares specific fields within JSON (JavaScript Object Notation) data. This matching can involve finding similarities or correspondences between fields in different JSON objects.",
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "JSON Multi-Field Match",
        "key": "json_multi_field_match",
        "direct_use": False,
        "settings_template": {
            "fields": {
                "label": "Fields to Compare",
                "type": "fields_tags_editor",  # Custom type - tag-based add/remove editor
                "required": True,
                "description": "Add fields to compare using dot notation for nested paths (e.g., user.name)",
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "required": True,
                "description": "Column name containing the expected JSON object",
                "ground_truth_key": True,
                "advanced": True,  # Hidden in advanced section
            },
        },
        "description": "Compares configured fields in expected JSON against LLM output. Each field becomes a separate metric (0 or 1), with an aggregate_score showing the percentage of matching fields. Useful for entity extraction validation.",
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "JSON Diff Match",
        "key": "auto_json_diff",
        "direct_use": False,
        "description": "Compares the generated JSON output to a ground truth JSON and returns a normalized score between 0 and 1 based on their differences.",
        "settings_template": {
            "compare_schema_only": {
                "label": "Compare Schema Only",
                "type": "boolean",
                "default": False,
                "advanced": True,
                "description": "If set to True, only the key names and their types will be compared between prediction and ground truth, ignoring the actual values. If set to False, key names, their types, and their values will all compared.",
            },
            "predict_keys": {
                "label": "Include prediction keys",
                "type": "boolean",
                "default": False,
                "advanced": True,
                "description": "If set to True, only keys present in the ground truth will be considered. The result will be 1.0 if a key from the ground truth is correctly predicted, regardless of any additional predicted keys. Otherwise both ground truth and prediction keys will be checked.",
            },
            "case_insensitive_keys": {
                "label": "Enable Case-sensitive keys",
                "type": "boolean",
                "default": False,
                "advanced": True,
                "description": "If set to True, keys will be treated as case-insensitive, meaning 'key', 'Key', and 'KEY' are considered equivalent. Otherwise, keys will be treated as case-sensitive.",
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Semantic Similarity Match",
        "key": "auto_semantic_similarity",
        "direct_use": False,
        "requires_llm_api_keys": True,
        "description": "Semantic Similarity Match evaluator measures the similarity between two pieces of text by analyzing their meaning and context. It compares the semantic content, providing a score that reflects how closely the texts match in terms of meaning, rather than just exact word matches.",
        "settings_template": {
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "required": True,
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["similarity"],
    },
    {
        "name": "Webhook test",
        "key": "auto_webhook_test",
        "direct_use": False,
        "settings_template": {
            "requires_llm_api_keys": {
                "label": "Requires LLM API Key(s)",
                "type": "boolean",
                "required": True,
                "default": False,
                "advanced": True,
                "description": "Indicates whether the evaluation requires LLM API key(s) to function.",
            },
            "webhook_url": {
                "label": "Webhook URL",
                "type": "string",
                "description": "https://your-webhook-url.com",
                "required": True,
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "required": False,
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "description": "Webhook test evaluator sends the generated answer and the correct_answer to a webhook and expects a response, in JSON format, indicating the correctness of the answer, along with a 200 HTTP status. You need to provide the URL of the webhook and the response of the webhook must be between 0 and 1.",
        "oss": True,
        "tags": ["custom"],
    },
    {
        "name": "Exact Match",
        "key": "auto_exact_match",
        "direct_use": True,
        "settings_template": {
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "description": "Exact Match evaluator determines if the output exactly matches the specified correct answer, ensuring precise alignment with expected results.",
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Contains JSON",
        "key": "auto_contains_json",
        "direct_use": True,
        "settings_template": {},
        "description": "'Contains JSON' evaluator checks if the output contains the a valid JSON.",
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Similarity Match",
        "key": "auto_similarity_match",
        "direct_use": False,
        "settings_template": {
            "similarity_threshold": {
                "label": "Similarity Threshold",
                "type": "number",
                "default": 0.5,
                "description": "The threshold value for similarity comparison",
                "min": 0,
                "max": 1,
                "required": True,
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "description": "Similarity Match evaluator checks if the generated answer is similar to the expected answer. You need to provide the similarity threshold. It uses the Jaccard similarity to compare the answers.",
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["similarity"],
    },
    {
        "name": "Regex Test",
        "key": "auto_regex_test",
        "direct_use": False,
        "description": "Regex Test evaluator checks if the generated answer matches a regular expression pattern. You need to provide the regex expression and specify whether an answer is correct if it matches or does not match the regex.",
        "settings_presets": [
            {
                "key": "starts_with",
                "name": "Starts With",
                "values": {
                    "regex_pattern": "^PREFIX_PLACEHOLDER",
                    "regex_should_match": True,
                },
                "description": "Checks if the output starts with a specified prefix. Replace PREFIX_PLACEHOLDER with your desired prefix. For case-insensitive matching, use (?i)^PREFIX_PLACEHOLDER",
            },
            {
                "key": "ends_with",
                "name": "Ends With",
                "values": {
                    "regex_pattern": "SUFFIX_PLACEHOLDER$",
                    "regex_should_match": True,
                },
                "description": "Checks if the output ends with a specified suffix. Replace SUFFIX_PLACEHOLDER with your desired suffix. For case-insensitive matching, use (?i)SUFFIX_PLACEHOLDER$",
            },
            {
                "key": "contains",
                "name": "Contains",
                "values": {
                    "regex_pattern": "SUBSTRING_PLACEHOLDER",
                    "regex_should_match": True,
                },
                "description": "Checks if the output contains a specified substring. Replace SUBSTRING_PLACEHOLDER with your desired substring. For case-insensitive matching, use (?i)SUBSTRING_PLACEHOLDER",
            },
            {
                "key": "contains_any",
                "name": "Contains Any",
                "values": {
                    "regex_pattern": "(OPTION1|OPTION2|OPTION3)",
                    "regex_should_match": True,
                },
                "description": "Checks if the output contains any of the specified substrings. Replace OPTION1, OPTION2, OPTION3 with your desired substrings separated by |. For case-insensitive matching, use (?i)(OPTION1|OPTION2|OPTION3)",
            },
            {
                "key": "contains_all",
                "name": "Contains All",
                "values": {
                    "regex_pattern": "(?=.*SUBSTRING1)(?=.*SUBSTRING2)(?=.*SUBSTRING3).*",
                    "regex_should_match": True,
                },
                "description": "Checks if the output contains all of the specified substrings. Replace SUBSTRING1, SUBSTRING2, SUBSTRING3 with your desired substrings. For case-insensitive matching, use (?i)(?=.*SUBSTRING1)(?=.*SUBSTRING2)(?=.*SUBSTRING3).*",
            },
        ],
        "settings_template": {
            "regex_pattern": {
                "label": "Regex Pattern",
                "type": "regex",
                "default": "",
                "description": "Pattern for regex testing (ex: ^this_word\\d{3}$)",
                "required": True,
            },
            "regex_should_match": {
                "label": "Match/Mismatch",
                "type": "boolean",
                "default": True,
                "description": "If the regex should match or mismatch",
            },
        },
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Starts With",
        "key": "auto_starts_with",
        "direct_use": False,
        "archived": True,
        "settings_template": {
            "prefix": {
                "label": "prefix",
                "type": "string",
                "required": True,
                "description": "The string to match at the start of the output.",
            },
            "case_sensitive": {
                "label": "Case Sensitive",
                "type": "boolean",
                "default": True,
                "description": "If the evaluation should be case sensitive.",
            },
        },
        "description": "Starts With evaluator checks if the output starts with a specified prefix, considering case sensitivity based on the settings.",
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Ends With",
        "key": "auto_ends_with",
        "direct_use": False,
        "archived": True,
        "settings_template": {
            "case_sensitive": {
                "label": "Case Sensitive",
                "type": "boolean",
                "default": True,
                "description": "If the evaluation should be case sensitive.",
            },
            "suffix": {
                "label": "suffix",
                "type": "string",
                "description": "The string to match at the end of the output.",
                "required": True,
            },
        },
        "description": "Ends With evaluator checks if the output ends with a specified suffix, considering case sensitivity based on the settings.",
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Contains",
        "key": "auto_contains",
        "direct_use": False,
        "archived": True,
        "settings_template": {
            "case_sensitive": {
                "label": "Case Sensitive",
                "type": "boolean",
                "default": True,
                "description": "If the evaluation should be case sensitive.",
            },
            "substring": {
                "label": "substring",
                "type": "string",
                "description": "The string to check if it is contained in the output.",
                "required": True,
            },
        },
        "description": "Contains evaluator checks if the output contains a specified substring, considering case sensitivity based on the settings.",
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Contains Any",
        "key": "auto_contains_any",
        "direct_use": False,
        "archived": True,
        "settings_template": {
            "case_sensitive": {
                "label": "Case Sensitive",
                "type": "boolean",
                "default": True,
                "description": "If the evaluation should be case sensitive.",
            },
            "substrings": {
                "label": "substrings",
                "type": "string",
                "description": "Provide a comma-separated list of strings to check if any is contained in the output.",
                "required": True,
            },
        },
        "description": "Contains Any evaluator checks if the output contains any of the specified substrings from a comma-separated list, considering case sensitivity based on the settings.",
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Contains All",
        "key": "auto_contains_all",
        "direct_use": False,
        "archived": True,
        "settings_template": {
            "case_sensitive": {
                "label": "Case Sensitive",
                "type": "boolean",
                "default": True,
                "description": "If the evaluation should be case sensitive.",
            },
            "substrings": {
                "label": "substrings",
                "type": "string",
                "description": "Provide a comma-separated list of strings to check if all are contained in the output.",
                "required": True,
            },
        },
        "description": "Contains All evaluator checks if the output contains all of the specified substrings from a comma-separated list, considering case sensitivity based on the settings.",
        "requires_testcase": "never",
        "requires_trace": "always",
        "oss": True,
        "tags": ["classifiers"],
    },
    {
        "name": "Levenshtein Distance",
        "key": "auto_levenshtein_distance",
        "direct_use": False,
        "settings_template": {
            "threshold": {
                "label": "Threshold",
                "type": "number",
                "required": False,
                "description": "The maximum allowed Levenshtein distance between the output and the correct answer.",
            },
            "correct_answer_key": {
                "label": "Expected Answer Column",
                "default": "correct_answer",
                "type": "string",
                "advanced": True,  # Tells the frontend that this setting is advanced and should be hidden by default
                "ground_truth_key": True,  # Tells the frontend that is the name of the column in the testset that should be shown as a ground truth to the user
                "description": "The name of the column in the test data that contains the correct answer",
            },
        },
        "description": "This evaluator calculates the Levenshtein distance between the output and the correct answer. If a threshold is provided in the settings, it returns a boolean indicating whether the distance is within the threshold. If no threshold is provided, it returns the actual Levenshtein distance as a numerical value.",
        "requires_testcase": "always",
        "requires_trace": "always",
        "oss": True,
        "tags": ["similarity"],
    },
    # {
    #     "name": "RAG Faithfulness",
    #     "key": "rag_faithfulness",
    #     "direct_use": False,
    #     "archived": True,
    #     "requires_llm_api_keys": True,
    #     "settings_template": rag_evaluator_settings_template,
    #     "description": "RAG Faithfulness evaluator assesses the accuracy and reliability of responses generated by Retrieval-Augmented Generation (RAG) models. It evaluates how faithfully the responses adhere to the retrieved documents or sources, ensuring that the generated text accurately reflects the information from the original sources.",
    #     "requires_testcase": "always",
    #     "requires_trace": "always",
    #     "tags": ["rag"],
    # },
    # {
    #     "name": "RAG Context Relevancy",
    #     "key": "rag_context_relevancy",
    #     "direct_use": False,
    #     "archived": True,
    #     "requires_llm_api_keys": True,
    #     "settings_template": rag_evaluator_settings_template,
    #     "description": "RAG Context Relevancy evaluator measures how relevant the retrieved documents or contexts are to the given question or prompt. It ensures that the selected documents provide the necessary information for generating accurate and meaningful responses, improving the overall quality of the RAG model's output.",
    #     "requires_testcase": "always",
    #     "requires_trace": "always",
    #     "tags": ["rag"],
    # },
]


_SUCCESS_ONLY_OUTPUT_SCHEMA = {
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "type": "object",
    "properties": {
        "success": {"type": "boolean"},
    },
    "required": ["success"],
    "additionalProperties": False,
}

_SCORE_AND_SUCCESS_OUTPUT_SCHEMA = {
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "type": "object",
    "properties": {
        "score": {"type": "number"},
        "success": {"type": "boolean"},
    },
    "required": [],
    "additionalProperties": False,
}

_FIXED_OUTPUT_SCHEMA_BY_KEY = {
    "auto_custom_code_run": _SCORE_AND_SUCCESS_OUTPUT_SCHEMA,
    "field_match_test": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_json_diff": _SCORE_AND_SUCCESS_OUTPUT_SCHEMA,
    "auto_semantic_similarity": _SCORE_AND_SUCCESS_OUTPUT_SCHEMA,
    "auto_webhook_test": _SCORE_AND_SUCCESS_OUTPUT_SCHEMA,
    "auto_exact_match": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_contains_json": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_similarity_match": _SCORE_AND_SUCCESS_OUTPUT_SCHEMA,
    "auto_regex_test": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_starts_with": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_ends_with": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_contains": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_contains_any": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_contains_all": _SUCCESS_ONLY_OUTPUT_SCHEMA,
    "auto_levenshtein_distance": _SCORE_AND_SUCCESS_OUTPUT_SCHEMA,
}


def _extract_auto_ai_critique_default_outputs_schema():
    for evaluator in evaluators:
        if evaluator.get("key") != "auto_ai_critique":
            continue

        settings_template = evaluator.get("settings_template") or {}
        json_schema_field = settings_template.get("json_schema") or {}
        default_value = json_schema_field.get("default") or {}
        schema = default_value.get("schema")

        return deepcopy(schema) if isinstance(schema, dict) else None

    return None


_auto_ai_critique_outputs_schema = _extract_auto_ai_critique_default_outputs_schema()
if _auto_ai_critique_outputs_schema is not None:
    _FIXED_OUTPUT_SCHEMA_BY_KEY["auto_ai_critique"] = _auto_ai_critique_outputs_schema

for evaluator in evaluators:
    evaluator_key = evaluator.get("key")
    outputs_schema = (
        _FIXED_OUTPUT_SCHEMA_BY_KEY.get(evaluator_key)
        if isinstance(evaluator_key, str)
        else None
    )
    if outputs_schema is not None:
        evaluator["outputs_schema"] = deepcopy(outputs_schema)


def get_all_evaluators():
    """
    Returns a list of evaluators

    Returns:
        List[dict]: A list of evaluator dictionaries.
    """
    return evaluators
