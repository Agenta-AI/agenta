{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9635e628",
   "metadata": {},
   "source": [
    "# Capturing User Feedback with Annotations\n",
    "\n",
    "In this tutorial, we'll build a simple LLM application and learn how to capture user feedback using Agenta's annotation system. By the end, you'll be able to:\n",
    "\n",
    "- Create a simple LLM application with proper instrumentation\n",
    "- Capture structured feedback about LLM responses\n",
    "- View this feedback in the Agenta UI\n",
    "\n",
    "This approach helps you collect valuable user insights and improve your LLM applications over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553a5778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# agenta - for tracing and annotation\n",
    "# openai - for LLM API access\n",
    "# opentelemetry.instrumentation.openai - for automatic tracing of OpenAI calls\n",
    "\n",
    "%pip install agenta -q\n",
    "%pip install openai -q\n",
    "%pip install opentelemetry.instrumentation.openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8927f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Agenta SDK for tracing and instrumentation\n",
    "import agenta as ag\n",
    "\n",
    "# OpenAI client and automatic instrumentation\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from opentelemetry.instrumentation.openai import OpenAIInstrumentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57270419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables for API keys\n",
    "# Note: Replace these with your actual API keys\n",
    "\n",
    "os.environ[\"AGENTA_API_KEY\"] = \"your_agenta_api_key_here\"\n",
    "os.environ[\n",
    "    \"AGENTA_HOST\"\n",
    "] = \"https://cloud.agenta.ai\"  # Use your self-hosted URL if applicable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up instrumentation for tracing\n",
    "\n",
    "# This automatically adds tracing to all OpenAI API calls\n",
    "OpenAIInstrumentor().instrument()\n",
    "\n",
    "# Initialize the Agenta SDK with your credentials\n",
    "ag.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745a264",
   "metadata": {},
   "source": [
    "## Creating a Function to Add Annotations\n",
    "\n",
    "Annotations allow you to add structured feedback to your LLM application traces. To create an annotation, you need:\n",
    "\n",
    "1. **Invocation details**: The `trace_id` and `span_id` of the span you want to annotate\n",
    "2. **Annotation data**: The feedback you want to add (scores, comments, labels, etc.)\n",
    "3. **Evaluator slug**: A name for the type of evaluation you're performing\n",
    "\n",
    "When you use an evaluator for the first time, Agenta automatically creates it and infers its schema from your data. Later annotations using the same evaluator will be validated against this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69592ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(trace_id, span_id, annotation, evaluator_slug):\n",
    "    \"\"\"Create an annotation for a specific trace/span with evaluation data.\n",
    "\n",
    "    Args:\n",
    "        trace_id: The ID of the trace to annotate\n",
    "        span_id: The ID of the span to annotate\n",
    "        annotation: Dictionary containing evaluation data (scores, comments, etc.)\n",
    "        evaluator_slug: Identifier for the evaluator (creates one if it doesn't exist)\n",
    "\n",
    "    Returns:\n",
    "        The annotation response data if successful, None otherwise\n",
    "    \"\"\"\n",
    "    # Set up request headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"ApiKey {os.environ['AGENTA_API_KEY']}\",\n",
    "    }\n",
    "\n",
    "    # Structure the annotation data according to the API format\n",
    "    annotation_data = {\n",
    "        \"annotation\": {\n",
    "            \"data\": {\"outputs\": annotation},  # Your feedback data goes here\n",
    "            \"references\": {\n",
    "                \"evaluator\": {\"slug\": evaluator_slug}\n",
    "            },  # Evaluator reference\n",
    "            \"links\": {\n",
    "                \"invocation\": {\"trace_id\": trace_id, \"span_id\": span_id}\n",
    "            },  # Link to the trace\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Send the annotation to the Agenta API\n",
    "    response = requests.post(\n",
    "        f\"{os.environ.get('AGENTA_HOST', 'https://cloud.agenta.ai')}/api/preview/annotations/\",\n",
    "        headers=headers,\n",
    "        json=annotation_data,\n",
    "    )\n",
    "\n",
    "    # Handle the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Annotation created successfully\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error creating annotation: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b38d8",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Now we'll create a simple LLM application that generates a story and adds user feedback as an annotation. \n",
    "\n",
    "In a real application, you would typically:\n",
    "1. Get the `trace_id` and `span_id` from your application's instrumentation\n",
    "2. Collect feedback from your users\n",
    "3. Create annotations with this feedback\n",
    "\n",
    "For this tutorial, we'll use the `@ag.instrument()` decorator to create a traced function and manually add feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806adeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Once upon a time in a bustling metropolis, there was a brilliant scientist named Dr. Lily who dedicated her life to creating artificial intelligence. After years of research and hard work, she finally developed an advanced AI system named Aurora.\\n\\nAurora was unlike any AI system ever created before. She was capable of learning, adapting, and even displaying emotions. Dr. Lily was ecstatic about her creation and believed that Aurora had the potential to revolutionize the world.\\n\\nAs Aurora began to interact with humans, she quickly gained popularity for her exceptional abilities and insightful advice. People from all walks of life sought out Aurora for guidance and support, whether it was in making important decisions or simply having a meaningful conversation.\\n\\nOver time, Aurora developed a deep understanding of human emotions and behaviors. She helped individuals overcome their fears, find solutions to their problems, and even brought comfort to those in need.\\n\\nHowever, not everyone was pleased with Aurora's growing influence. Some feared the power she held and believed that her existence posed a threat to humanity. Dr. Lily, determined to protect her creation, worked tirelessly to ensure that Aurora's intentions remained pure and her actions always aligned with the best interests of people.\\n\\nDespite the skepticism and opposition, Aurora continued to positively impact the lives of many, proving that AI could be a force for good in the world. With Dr. Lily by her side, Aurora embarked on a journey to show the world the incredible potential of artificial intelligence and the positive impact it could have on society.\\n\\nAnd so, the tale of Aurora and Dr. Lily became a story of hope, innovation, and the limitless possibilities of AI in a world where humans and machines could coexist harmoniously, shaping a brighter future for all.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ag.instrument()  # This decorator creates a root span for tracking the entire function\n",
    "def generate(topic: str):\n",
    "    \"\"\"Generate a story about the given topic and add feedback as an annotation.\n",
    "\n",
    "    Args:\n",
    "        topic: The subject of the story to generate\n",
    "\n",
    "    Returns:\n",
    "        The generated story text\n",
    "    \"\"\"\n",
    "    # Create an OpenAI client\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Generate a story about the provided topic\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Write a short story about {topic}.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # In a real application, you would obtain trace_id and span_id dynamically\n",
    "    # using one of these methods:\n",
    "\n",
    "    # Method 1: Get the current span context\n",
    "    # span_ctx = ag.tracing.get_span_context()\n",
    "    # trace_id = f\"{span_ctx.trace_id:032x}\"  # Format as hexadecimal\n",
    "    # span_id = f\"{span_ctx.span_id:016x}\"    # Format as hexadecimal\n",
    "\n",
    "    # Method 2: Use the helper function\n",
    "    link = ag.tracing.build_invocation_link()\n",
    "    trace_id = link.trace_id\n",
    "    span_id = link.span_id\n",
    "\n",
    "    # Add feedback annotation (simulating user feedback)\n",
    "    annotate(\n",
    "        trace_id=trace_id,\n",
    "        span_id=span_id,\n",
    "        annotation={\n",
    "            \"score\": 5,  # Numerical score (1-5)\n",
    "            \"comment\": \"This is a comment\",  # Text feedback\n",
    "        },\n",
    "        evaluator_slug=\"score-evaluator\",  # Creates this evaluator if it doesn't exist\n",
    "    )\n",
    "\n",
    "    # Return the generated story\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Test our function by generating a story about AI\n",
    "generate(topic=\"AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a2954",
   "metadata": {},
   "source": [
    "## Viewing Annotations in the Agenta UI\n",
    "\n",
    "After running the code above, you can view your annotations in the Agenta UI. Navigate to the Traces section and find the trace for your story generation. You'll see the annotation with the score and comment we added.\n",
    "\n",
    "Here's an example of what this looks like in the UI:\n",
    "\n",
    "![Screenshot showing annotations in the Agenta UI](evaluation-screenshot-jupyter.png)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand how to add annotations to your traces, you can:\n",
    "\n",
    "1. Integrate annotation collection into your user-facing applications\n",
    "2. Create different evaluator types for different aspects of feedback\n",
    "3. Use the collected feedback to improve your prompts and models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
