{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Annotate Traces Tutorial\n",
    "\n",
    "Annotations in Agenta let you enrich the traces created by your LLM applications. You can add scores, comments, expected answers and other metrics to help evaluate your application's performance.\n",
    "\n",
    "In this tutorial, we'll:\n",
    "1. Set up the Agenta SDK and create a traced LLM application\n",
    "2. Run the application to generate traces\n",
    "3. Add annotations to those traces programmatically\n",
    "4. Query and view the annotations\n",
    "\n",
    "## What You Can Do With Annotations\n",
    "\n",
    "- Collect user feedback on LLM responses\n",
    "- Run custom evaluation workflows\n",
    "- Measure application performance in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "First, install the Agenta SDK, OpenAI, and the OpenTelemetry instrumentor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U agenta openai opentelemetry-instrumentation-openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Step 2: Configure Environment Variables\n",
    "\n",
    "To start tracing your application and adding annotations, you'll need an API key:\n",
    "\n",
    "1. Visit the Agenta API Keys page under settings\n",
    "2. Click on **Create New API Key** and follow the prompts\n",
    "\n",
    "Then set your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API keys here\n",
    "os.environ[\"AGENTA_API_KEY\"] = \"\"\n",
    "os.environ[\"AGENTA_HOST\"] = \"https://cloud.agenta.ai\"  # Change for self-hosted\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-sdk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agenta as ag\n",
    "from getpass import getpass\n",
    "\n",
    "# Initialize the SDK with your API key\n",
    "api_key = os.getenv(\"AGENTA_API_KEY\")\n",
    "if not api_key:\n",
    "    os.environ[\"AGENTA_API_KEY\"] = getpass(\"Enter your Agenta API key: \")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Initialize Agenta\n",
    "ag.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrument",
   "metadata": {},
   "source": [
    "## Step 3: Create and Instrument an LLM Application\n",
    "\n",
    "Let's create a simple LLM application that we can trace and annotate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-openai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "# Instrument OpenAI to automatically capture traces\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.instrument()\n",
    "def answer_question(question: str) -> tuple[str, str, str]:\n",
    "    \"\"\"A simple question-answering function that we'll trace and annotate.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (answer, trace_id, span_id)\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Automatically get the trace_id and span_id from the current span\n",
    "    link = ag.tracing.build_invocation_link()\n",
    "    \n",
    "    return response.choices[0].message.content, link.trace_id, link.span_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-trace",
   "metadata": {},
   "source": [
    "## Step 4: Generate a Trace\n",
    "\n",
    "Let's run our function to generate a trace. The function will automatically capture the trace_id and span_id using `ag.tracing.build_invocation_link()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to create a trace and get the IDs automatically\n",
    "question = \"What is the capital of France?\"\n",
    "result, trace_id, span_id = answer_question(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result}\")\n",
    "print(f\"\\n✅ Trace captured!\")\n",
    "print(f\"Trace ID: {trace_id}\")\n",
    "print(f\"Span ID: {span_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-annotation",
   "metadata": {},
   "source": [
    "## Step 5: Create an Annotation\n",
    "\n",
    "Now let's add an annotation to the trace we just created. We'll use the trace_id and span_id that were automatically captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annotate-trace",
   "metadata": {},
   "outputs": [],
   "source": "import requests\n\nbase_url = os.environ.get(\"AGENTA_HOST\", \"https://cloud.agenta.ai\")\napi_key = os.environ[\"AGENTA_API_KEY\"]\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"ApiKey {api_key}\"\n}\n\n# Create an annotation with a score and reasoning\nannotation_data = {\n    \"annotation\": {\n        \"data\": {\n            \"outputs\": {\n                \"score\": 90,\n                \"normalized_score\": 0.9,\n                \"reasoning\": \"The answer is correct and concise\",\n                \"expected_answer\": \"The capital of France is Paris\"\n            }\n        },\n        \"references\": {\n            \"evaluator\": {\n                \"slug\": \"accuracy_evaluator\"\n            }\n        },\n        \"links\": {\n            \"invocation\": {\n                \"trace_id\": trace_id,\n                \"span_id\": span_id\n            }\n        },\n        \"metadata\": {\n            \"annotator\": \"tutorial_user\",\n            \"timestamp\": \"2025-10-30T00:00:00Z\"\n        }\n    }\n}\n\n# Make the API request (note the trailing slash!)\nresponse = requests.post(\n    f\"{base_url}/api/preview/annotations/\",\n    headers=headers,\n    json=annotation_data\n)\n\n# Process the response\nif response.status_code == 200:\n    print(\"✅ Annotation created successfully!\")\n    annotation_response = response.json()\n    print(f\"\\nAnnotation ID: {annotation_response['annotation']['trace_id']}\")\n    print(f\"Span ID: {annotation_response['annotation']['span_id']}\")\n    print(f\"\\nAnnotation data:\")\n    print(annotation_response)\nelse:\n    print(f\"❌ Error: {response.status_code}\")\n    print(response.text)"
  },
  {
   "cell_type": "markdown",
   "id": "multiple-annotations",
   "metadata": {},
   "source": [
    "## Step 6: Create Additional Annotations\n",
    "\n",
    "You can add multiple annotations to the same trace from different evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-second-annotation",
   "metadata": {},
   "outputs": [],
   "source": "# Create another annotation for quality assessment\nquality_annotation = {\n    \"annotation\": {\n        \"data\": {\n            \"outputs\": {\n                \"score\": 85,\n                \"reasoning\": \"Response is helpful and well-formatted\",\n                \"labels\": [\"Helpful\", \"Accurate\", \"Concise\"]\n            }\n        },\n        \"references\": {\n            \"evaluator\": {\n                \"slug\": \"quality_evaluator\"\n            }\n        },\n        \"links\": {\n            \"invocation\": {\n                \"trace_id\": trace_id,\n                \"span_id\": span_id\n            }\n        }\n    }\n}\n\nresponse = requests.post(\n    f\"{base_url}/api/preview/annotations/\",  # Note the trailing slash!\n    headers=headers,\n    json=quality_annotation\n)\n\nif response.status_code == 200:\n    print(\"✅ Quality annotation created successfully!\")\nelse:\n    print(f\"❌ Error: {response.status_code}\")\n    print(response.text)"
  },
  {
   "cell_type": "markdown",
   "id": "query-annotations",
   "metadata": {},
   "source": [
    "## Step 7: Query Annotations\n",
    "\n",
    "Now let's query all annotations for our invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-by-invocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all annotations for the invocation\n",
    "query_data = {\n",
    "    \"annotation\": {\n",
    "        \"links\": {\n",
    "            \"invocation\": {\n",
    "                \"trace_id\": trace_id,\n",
    "                \"span_id\": span_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{base_url}/api/preview/annotations/query\",\n",
    "    headers=headers,\n",
    "    json=query_data\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"✅ Annotations retrieved successfully!\")\n",
    "    annotations = response.json()\n",
    "    print(f\"\\nFound {len(annotations.get('annotations', []))} annotation(s)\")\n",
    "    print(\"\\nAnnotations:\")\n",
    "    for idx, ann in enumerate(annotations.get('annotations', []), 1):\n",
    "        print(f\"\\n--- Annotation {idx} ---\")\n",
    "        print(f\"Evaluator: {ann['references']['evaluator']['slug']}\")\n",
    "        print(f\"Data: {ann['data']}\")\n",
    "else:\n",
    "    print(f\"❌ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view-ui",
   "metadata": {},
   "source": [
    "## Step 8: View Annotations in the UI\n",
    "\n",
    "You can see all annotations for a trace in the Agenta UI:\n",
    "\n",
    "1. Log in to your Agenta dashboard\n",
    "2. Navigate to the **Observability** section\n",
    "3. Find your trace\n",
    "4. Check the **Annotations** tab to see detailed information\n",
    "\n",
    "The right sidebar will show average metrics for each evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-structure",
   "metadata": {},
   "source": [
    "## Understanding Automatic Trace Capture\n",
    "\n",
    "The `ag.tracing.build_invocation_link()` function is a helper that automatically:\n",
    "1. Gets the current span context from the active trace\n",
    "2. Formats the trace_id and span_id as hex strings\n",
    "3. Returns a Link object with both IDs ready to use\n",
    "\n",
    "This is much more convenient than manually querying the UI for trace IDs!\n",
    "\n",
    "**Alternative Method:**\n",
    "You can also use `ag.tracing.get_span_context()` if you need more control:\n",
    "\n",
    "```python\n",
    "span_ctx = ag.tracing.get_span_context()\n",
    "trace_id = f\"{span_ctx.trace_id:032x}\"  # Format as hexadecimal\n",
    "span_id = f\"{span_ctx.span_id:016x}\"    # Format as hexadecimal\n",
    "```\n",
    "\n",
    "## Understanding Annotation Structure\n",
    "\n",
    "An annotation has four main parts:\n",
    "\n",
    "1. **Data**: The actual evaluation content (scores, comments)\n",
    "2. **References**: Which evaluator to use (will be created automatically if it doesn't exist)\n",
    "3. **Links**: Which trace and span you're annotating\n",
    "4. **Metadata** (optional): Any extra information you want to include\n",
    "\n",
    "### Annotation Data Examples\n",
    "\n",
    "You can include various types of data in your annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annotation-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple score\n",
    "simple_annotation = {\n",
    "    \"outputs\": {\n",
    "        \"score\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example 2: Score with explanation\n",
    "detailed_annotation = {\n",
    "    \"outputs\": {\n",
    "        \"score\": 3,\n",
    "        \"comment\": \"The response is not grounded\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example 3: Multiple metrics with reference information\n",
    "comprehensive_annotation = {\n",
    "    \"outputs\": {\n",
    "        \"score\": 3,\n",
    "        \"normalized_score\": 0.5,\n",
    "        \"comment\": \"The response is not grounded\",\n",
    "        \"expected_answer\": \"The capital of France is Paris\",\n",
    "        \"labels\": [\"factual\", \"concise\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Annotation data can include:\")\n",
    "print(\"- Numbers (scores, ratings)\")\n",
    "print(\"- Categories (labels, classifications)\")\n",
    "print(\"- Text (comments, reasoning)\")\n",
    "print(\"- Booleans (true/false values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Optional: Remove an Annotation\n",
    "\n",
    "If you need to remove an annotation, you can delete it by its trace_id and span_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and replace with your annotation's trace_id and span_id to delete\n",
    "# annotation_trace_id = \"your_annotation_trace_id\"\n",
    "# annotation_span_id = \"your_annotation_span_id\"\n",
    "\n",
    "# response = requests.delete(\n",
    "#     f\"{base_url}/api/preview/annotations/{annotation_trace_id}/{annotation_span_id}\",\n",
    "#     headers=headers\n",
    "# )\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     print(\"✅ Annotation deleted successfully\")\n",
    "# else:\n",
    "#     print(f\"❌ Error: {response.status_code}\")\n",
    "#     print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. ✅ Setting up the Agenta SDK and instrumenting an LLM application\n",
    "2. ✅ Generating traces by running the application\n",
    "3. ✅ Creating annotations with scores, reasoning, and metadata\n",
    "4. ✅ Adding multiple annotations from different evaluators\n",
    "5. ✅ Querying annotations programmatically\n",
    "6. ✅ Understanding annotation structure and capabilities\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you know how to annotate traces, you can:\n",
    "\n",
    "- Integrate annotation creation into your evaluation workflows\n",
    "- Build custom evaluators that automatically annotate traces\n",
    "- Use annotations to track user feedback in production\n",
    "- Analyze annotation data to improve your LLM applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}