{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace with Python SDK - Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the Agenta Python SDK to trace your LLM applications. You'll learn how to:\n",
    "\n",
    "- Set up tracing with the Agenta SDK\n",
    "- Instrument functions and OpenAI calls automatically\n",
    "- Start and end spans manually to capture internals\n",
    "- Reference prompt versions in your traces\n",
    "- Redact sensitive data from traces\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "We'll create a simple LLM application that:\n",
    "1. Uses OpenAI auto-instrumentation to trace API calls\n",
    "2. Instruments custom functions to capture workflow steps\n",
    "3. Stores internal data like retrieved context\n",
    "4. Links traces to deployed prompt versions\n",
    "5. Redacts sensitive information from traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U agenta openai opentelemetry-instrumentation-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before using the SDK, we need to initialize it with your API keys. The SDK requires:\n",
    "- **Agenta API Key**: For sending traces to Agenta\n",
    "- **OpenAI API Key**: For making LLM calls\n",
    "\n",
    "You can get your Agenta API key from the [API Keys page](https://cloud.agenta.ai/settings?tab=apiKeys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AGENTA_HOST\"] = \"https://cloud.agenta.ai/\"  # Default value, change for self-hosted\n",
    "os.environ[\"AGENTA_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import agenta as ag\n",
    "from getpass import getpass\n",
    "\n",
    "# Initialize the SDK with your API key\n",
    "api_key = os.getenv(\"AGENTA_API_KEY\")\n",
    "if not api_key:\n",
    "    os.environ[\"AGENTA_API_KEY\"] = getpass(\"Enter your Agenta API key: \")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Initialize the SDK\n",
    "ag.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup Tracing with OpenAI Auto-Instrumentation\n",
    "\n",
    "The Agenta SDK provides two powerful mechanisms for tracing:\n",
    "\n",
    "1. **Auto-instrumentation**: Automatically traces third-party libraries like OpenAI\n",
    "2. **Function decorators**: Manually instrument your custom functions\n",
    "\n",
    "Let's start by setting up OpenAI auto-instrumentation, which will capture all OpenAI API calls automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
    "import openai\n",
    "\n",
    "# Instrument OpenAI to automatically trace all API calls\n",
    "OpenAIInstrumentor().instrument()\n",
    "\n",
    "print(\"OpenAI auto-instrumentation enabled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Instrument Functions\n",
    "\n",
    "Now let's create a simple function and instrument it using the `@ag.instrument()` decorator. This will create a span for the function and automatically capture its inputs and outputs.\n",
    "\n",
    "The decorator accepts a `spankind` parameter to categorize the span. Available types include: `agent`, `chain`, `workflow`, `tool`, `embedding`, `query`, `completion`, `chat`, `rerank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.instrument(spankind=\"workflow\")\n",
    "def generate_story(topic: str):\n",
    "    \"\"\"Generate a short story about the given topic.\"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Write a short story about {topic}.\"},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the instrumented function\n",
    "story = generate_story(\"AI Engineering\")\n",
    "print(\"Generated story:\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Starting Spans and Storing Internals\n",
    "\n",
    "Sometimes you need to capture intermediate data that isn't part of the function's inputs or outputs. The SDK provides two methods:\n",
    "\n",
    "- `ag.tracing.store_meta()`: Add metadata to a span (saved under `ag.meta`)\n",
    "- `ag.tracing.store_internals()`: Store internal data (saved under `ag.data.internals`)\n",
    "\n",
    "Internals are especially useful because they:\n",
    "1. Are searchable using plain text queries\n",
    "2. Appear in the overview tab of the observability drawer\n",
    "\n",
    "Let's create a RAG (Retrieval-Augmented Generation) example that captures the retrieved context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.instrument(spankind=\"tool\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Simulate retrieving context from a knowledge base.\"\"\"\n",
    "    # In a real application, this would query a vector database\n",
    "    context = [\n",
    "        \"Agenta is an open-source LLM developer platform.\",\n",
    "        \"Agenta provides tools for prompt management, evaluation, and observability.\",\n",
    "        \"The Agenta SDK supports tracing with OpenTelemetry.\",\n",
    "    ]\n",
    "    \n",
    "    # Store metadata about the retrieval\n",
    "    ag.tracing.store_meta({\n",
    "        \"retrieval_method\": \"vector_search\",\n",
    "        \"num_results\": len(context)\n",
    "    })\n",
    "    \n",
    "    return context\n",
    "\n",
    "@ag.instrument(spankind=\"workflow\")\n",
    "def rag_workflow(query: str):\n",
    "    \"\"\"Answer a question using retrieved context.\"\"\"\n",
    "    # Retrieve context\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Store the retrieved context as internals\n",
    "    # This makes it visible in the UI and searchable\n",
    "    ag.tracing.store_internals({\"retrieved_context\": context})\n",
    "    \n",
    "    # Generate answer using context\n",
    "    context_str = \"\\n\".join(context)\n",
    "    prompt = f\"Answer the following question based on the context:\\n\\nContext:\\n{context_str}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer questions based only on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the RAG workflow\n",
    "answer = rag_workflow(\"What is Agenta?\")\n",
    "print(\"Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Reference Prompt Versions\n",
    "\n",
    "One of Agenta's powerful features is linking traces to specific prompt versions. This allows you to:\n",
    "- Filter traces by application, variant, or environment\n",
    "- Compare performance across different variants\n",
    "- Track production behavior\n",
    "\n",
    "Let's create a prompt using the SDK, deploy it, and then reference it in our traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Deploy a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from agenta.sdk.types import PromptTemplate, Message, ModelConfig\nfrom pydantic import BaseModel\n\n# Define the prompt configuration\nclass Config(BaseModel):\n    prompt: PromptTemplate\n\nconfig = Config(\n    prompt=PromptTemplate(\n        messages=[\n            Message(role=\"system\", content=\"You are a helpful assistant that explains topics clearly.\"),\n            Message(role=\"user\", content=\"Explain {{topic}} in simple terms.\"),\n        ],\n        llm_config=ModelConfig(\n            model=\"gpt-3.5-turbo\",\n            max_tokens=200,\n            temperature=0.7,\n            top_p=1.0,\n            frequency_penalty=0.0,\n            presence_penalty=0.0,\n        ),\n        template_format=\"curly\"\n    )\n)\n\n# Create an application and variant\napp = ag.AppManager.create(\n    app_slug=\"topic-explainer-traced\",\n    template_key=\"SERVICE:completion\",\n)\n\nprint(f\"Created application: {app.app_name}\")\n\n# Create a variant with the prompt configuration\nvariant = ag.VariantManager.create(\n    parameters=config.model_dump(),\n    app_slug=\"topic-explainer-traced\",\n    variant_slug=\"production-variant\"\n)\n\nprint(f\"Created variant: {variant.variant_slug} (version {variant.variant_version})\")\n\n# Deploy to production environment\ndeployment = ag.DeploymentManager.deploy(\n    app_slug=\"topic-explainer-traced\",\n    variant_slug=\"production-variant\",\n    environment_slug=\"production\",\n)\n\nprint(f\"Deployed to {deployment.environment_slug} environment\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference the Prompt in Traces\n",
    "\n",
    "Now we'll create a function that uses the deployed prompt and links its traces to the application and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.instrument(spankind=\"workflow\")\n",
    "def explain_topic_with_prompt(topic: str):\n",
    "    \"\"\"Explain a topic using the deployed prompt configuration.\"\"\"\n",
    "    \n",
    "    # Fetch the prompt configuration from production\n",
    "    prompt_config = ag.ConfigManager.get_from_registry(\n",
    "        app_slug=\"topic-explainer-traced\",\n",
    "        environment_slug=\"production\"\n",
    "    )\n",
    "    \n",
    "    # Format the prompt with the topic\n",
    "    prompt_template = PromptTemplate(**prompt_config[\"prompt\"])\n",
    "    formatted_prompt = prompt_template.format(topic=topic)\n",
    "    \n",
    "    # Make the OpenAI call\n",
    "    response = openai.chat.completions.create(\n",
    "        **formatted_prompt.to_openai_kwargs()\n",
    "    )\n",
    "    \n",
    "    # Link this trace to the application and environment\n",
    "    ag.tracing.store_refs({\n",
    "        \"application.slug\": \"topic-explainer-traced\",\n",
    "        \"variant.slug\": \"production-variant\",\n",
    "        \"environment.slug\": \"production\",\n",
    "    })\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the function\n",
    "explanation = explain_topic_with_prompt(\"machine learning\")\n",
    "print(\"Explanation:\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Redact Sensitive Data\n",
    "\n",
    "When working with production data, you often need to exclude sensitive information from traces. The Agenta SDK provides several ways to redact data:\n",
    "\n",
    "1. **Simple redaction**: Ignore all inputs/outputs\n",
    "2. **Selective redaction**: Ignore specific fields\n",
    "3. **Custom redaction**: Use a callback function for fine-grained control\n",
    "4. **Global redaction**: Apply rules across all instrumented functions\n",
    "\n",
    "Let's explore these different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Redaction: Ignore All Inputs/Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.instrument(\n",
    "    spankind=\"workflow\",\n",
    "    ignore_inputs=True,\n",
    "    ignore_outputs=True\n",
    ")\n",
    "def process_sensitive_data(user_email: str, credit_card: str):\n",
    "    \"\"\"Process sensitive data without logging inputs/outputs.\"\"\"\n",
    "    # The function inputs and outputs won't be captured in the trace\n",
    "    result = f\"Processed data for {user_email}\"\n",
    "    return result\n",
    "\n",
    "# This trace will not contain inputs or outputs\n",
    "result = process_sensitive_data(\"user@example.com\", \"4111-1111-1111-1111\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Redaction: Ignore Specific Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.instrument(\n",
    "    spankind=\"workflow\",\n",
    "    ignore_inputs=[\"api_key\", \"password\"],\n",
    "    ignore_outputs=[\"internal_token\"]\n",
    ")\n",
    "def authenticate_user(username: str, password: str, api_key: str):\n",
    "    \"\"\"Authenticate a user (password and api_key will be redacted).\"\"\"\n",
    "    # Simulate authentication\n",
    "    return {\n",
    "        \"username\": username,\n",
    "        \"authenticated\": True,\n",
    "        \"internal_token\": \"secret-token-12345\",  # This will be redacted\n",
    "    }\n",
    "\n",
    "# The trace will show username but not password or api_key\n",
    "auth_result = authenticate_user(\"john_doe\", \"secret123\", \"sk-abc123\")\n",
    "print(f\"Authenticated: {auth_result['authenticated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Redaction: Use a Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def redact_pii(name: str, field: str, data: dict):\n",
    "    \"\"\"Custom redaction function that removes PII.\"\"\"\n",
    "    if field == \"inputs\":\n",
    "        # Redact email addresses\n",
    "        if \"email\" in data:\n",
    "            data[\"email\"] = \"[REDACTED]\"\n",
    "        # Redact phone numbers\n",
    "        if \"phone\" in data:\n",
    "            data[\"phone\"] = \"[REDACTED]\"\n",
    "    \n",
    "    if field == \"outputs\":\n",
    "        # Redact any credit card patterns\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, str):\n",
    "                    # Simple credit card pattern\n",
    "                    data[key] = re.sub(r'\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}', '[CARD-REDACTED]', value)\n",
    "    \n",
    "    return data\n",
    "\n",
    "@ag.instrument(\n",
    "    spankind=\"workflow\",\n",
    "    redact=redact_pii,\n",
    "    redact_on_error=False  # Don't apply redaction if it raises an error\n",
    ")\n",
    "def process_customer_order(name: str, email: str, phone: str, card_number: str):\n",
    "    \"\"\"Process a customer order with PII redaction.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"processed\",\n",
    "        \"customer\": name,\n",
    "        \"payment_info\": f\"Charged card ending in {card_number[-4:]}\",\n",
    "        \"full_card\": card_number  # This will be redacted\n",
    "    }\n",
    "\n",
    "# Test with sample data\n",
    "order = process_customer_order(\n",
    "    name=\"Jane Smith\",\n",
    "    email=\"jane@example.com\",  # Will be redacted\n",
    "    phone=\"555-1234\",  # Will be redacted\n",
    "    card_number=\"4111-1111-1111-1111\"  # Will be redacted in output\n",
    ")\n",
    "print(f\"Order status: {order['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Redaction: Apply Rules Across All Functions\n",
    "\n",
    "For organization-wide policies, you can set up global redaction rules during initialization. Note: Since we already called `ag.init()`, this is just for demonstration. In a real application, you would set this during the initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of global redaction setup (would be done during ag.init())\n",
    "from typing import Dict, Any\n",
    "\n",
    "def global_redact_function(name: str, field: str, data: Dict[str, Any]):\n",
    "    \"\"\"Global redaction that applies to all instrumented functions.\"\"\"\n",
    "    # Remove any field containing 'api_key' or 'secret'\n",
    "    if isinstance(data, dict):\n",
    "        keys_to_redact = [k for k in data.keys() if 'api_key' in k.lower() or 'secret' in k.lower()]\n",
    "        for key in keys_to_redact:\n",
    "            data[key] = \"[REDACTED]\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "# In production, you would initialize like this:\n",
    "# ag.init(\n",
    "#     redact=global_redact_function,\n",
    "#     redact_on_error=True\n",
    "# )\n",
    "\n",
    "print(\"Global redaction would be configured during ag.init()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. ✅ **Set up tracing** with the Agenta SDK and OpenAI auto-instrumentation\n",
    "2. ✅ **Instrument functions** using the `@ag.instrument()` decorator\n",
    "3. ✅ **Store internals and metadata** to capture intermediate data in your workflows\n",
    "4. ✅ **Reference prompt versions** by creating, deploying, and linking traces to applications\n",
    "5. ✅ **Redact sensitive data** using multiple approaches for privacy protection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore [distributed tracing](/observability/trace-with-python-sdk/distributed-tracing) for multi-service applications\n",
    "- Learn about [cost tracking](/observability/trace-with-python-sdk/track-costs) to monitor LLM expenses\n",
    "- Understand [trace annotations](/observability/trace-with-python-sdk/annotate-traces) for collecting feedback\n",
    "- Check out the [Agenta UI guide](/observability/using-the-ui/filtering-traces) for filtering and analyzing traces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}