{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenta SDK Quick Start - Evaluations\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Create a simple application that returns country capitals\n",
    "2. Create evaluators to check if the application's output is correct\n",
    "3. Run an evaluation to test your application\n",
    "\n",
    "The entire example takes less than 100 lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the Agenta SDK and set up your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Agenta SDK\n",
    "%pip install agenta -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom getpass import getpass\n\n# Set your API credentials\nif not os.getenv(\"AGENTA_API_KEY\"):\n    os.environ[\"AGENTA_API_KEY\"] = getpass(\"Enter your Agenta API key: \")\n\nif not os.getenv(\"AGENTA_HOST\"):\n    os.environ[\"AGENTA_HOST\"] = \"https://cloud.agenta.ai\"  # Change for self-hosted\n\n# Set OpenAI API key (required for LLM-as-a-judge evaluator)\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n\nprint(\"‚úÖ Environment configured!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Agenta SDK\n",
    "\n",
    "Initialize the SDK to connect to the Agenta platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agenta as ag\n",
    "\n",
    "ag.init()\n",
    "\n",
    "print(\"‚úÖ Agenta SDK initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Your Application\n",
    "\n",
    "An application is any function decorated with `@ag.application`. It receives inputs from test data and returns outputs.\n",
    "\n",
    "Let's create a simple application that returns country capitals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.application(\n",
    "    slug=\"capital_finder\",\n",
    "    name=\"Capital Finder\",\n",
    "    description=\"Returns the capital of a given country\"\n",
    ")\n",
    "async def capital_finder(country: str):\n",
    "    \"\"\"\n",
    "    A simple application that returns country capitals.\n",
    "    \n",
    "    Args:\n",
    "        country: The country name (from testcase)\n",
    "    \n",
    "    Returns:\n",
    "        The capital city name\n",
    "    \"\"\"\n",
    "    capitals = {\n",
    "        \"Germany\": \"Berlin\",\n",
    "        \"France\": \"Paris\",\n",
    "        \"Spain\": \"Madrid\",\n",
    "        \"Italy\": \"Rome\",\n",
    "    }\n",
    "    return capitals.get(country, \"Unknown\")\n",
    "\n",
    "print(\"‚úÖ Application defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Custom Evaluators\n",
    "\n",
    "Evaluators check if your application's output is correct. They receive:\n",
    "- Fields from your testcase (e.g., `capital`)\n",
    "- The application's output (always called `outputs`)\n",
    "\n",
    "Let's create two evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ag.evaluator(\n",
    "    slug=\"exact_match\",\n",
    "    name=\"Exact Match Evaluator\",\n",
    "    description=\"Checks if the output exactly matches the expected answer\"\n",
    ")\n",
    "async def exact_match(capital: str, outputs: str):\n",
    "    \"\"\"\n",
    "    Evaluates if the application's output matches the expected answer.\n",
    "    \n",
    "    Args:\n",
    "        capital: The expected capital (from testcase)\n",
    "        outputs: What the application returned\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with score and success flag\n",
    "    \"\"\"\n",
    "    is_correct = outputs == capital\n",
    "    return {\n",
    "        \"score\": 1.0 if is_correct else 0.0,\n",
    "        \"success\": is_correct,\n",
    "    }\n",
    "\n",
    "\n",
    "@ag.evaluator(\n",
    "    slug=\"case_insensitive_match\",\n",
    "    name=\"Case Insensitive Match\",\n",
    "    description=\"Checks if output matches ignoring case\"\n",
    ")\n",
    "async def case_insensitive_match(capital: str, outputs: str):\n",
    "    \"\"\"\n",
    "    Evaluates with case-insensitive comparison.\n",
    "    \"\"\"\n",
    "    is_correct = outputs.lower() == capital.lower()\n",
    "    return {\n",
    "        \"score\": 1.0 if is_correct else 0.0,\n",
    "        \"success\": is_correct,\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluators defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use Built-in Evaluators\n",
    "\n",
    "Agenta provides built-in evaluators like LLM-as-a-judge. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agenta.sdk.workflows import builtin\n",
    "\n",
    "llm_judge = builtin.auto_ai_critique(\n",
    "    slug=\"llm_judge\",\n",
    "    name=\"LLM Judge Evaluator\",\n",
    "    description=\"Uses an LLM to judge if the answer is correct\",\n",
    "    correct_answer_key=\"capital\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    prompt_template=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a geography expert evaluating answers about world capitals.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Expected capital: {{capital}}\\n\"\n",
    "                \"Student's answer: {{outputs}}\\n\\n\"\n",
    "                \"Is the student's answer correct?\\n\"\n",
    "                \"Respond with ONLY a number from 0.0 (wrong) to 1.0 (correct).\\n\"\n",
    "                \"Nothing else - just the number.\"\n",
    "            ),\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM judge evaluator created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Test Data\n",
    "\n",
    "Define test cases as a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\"country\": \"Germany\", \"capital\": \"Berlin\"},\n",
    "    {\"country\": \"France\", \"capital\": \"Paris\"},\n",
    "    {\"country\": \"Spain\", \"capital\": \"Madrid\"},\n",
    "    {\"country\": \"Italy\", \"capital\": \"Rome\"},\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(test_data)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Evaluation\n",
    "\n",
    "Now let's create a testset and run the evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agenta.sdk.evaluations import aevaluate\n",
    "\n",
    "# Create a testset\n",
    "print(\"üìù Creating testset...\")\n",
    "testset = await ag.testsets.acreate(\n",
    "    name=\"Country Capitals Quick Start\",\n",
    "    data=test_data,\n",
    ")\n",
    "\n",
    "if not testset or not testset.id:\n",
    "    print(\"‚ùå Failed to create testset\")\n",
    "else:\n",
    "    print(f\"‚úÖ Testset created with ID: {testset.id}\")\n",
    "    print(f\"   Contains {len(test_data)} test cases\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with all three evaluators\n",
    "print(\"üöÄ Running evaluation...\\n\")\n",
    "\n",
    "result = await aevaluate(\n",
    "    testsets=[testset.id],\n",
    "    applications=[capital_finder],\n",
    "    evaluators=[\n",
    "        exact_match,\n",
    "        case_insensitive_match,\n",
    "        llm_judge,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Evaluation Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "The evaluation results are now available in the Agenta UI! You can:\n",
    "\n",
    "1. **View detailed results** - See how each test case performed\n",
    "2. **Compare evaluators** - See which evaluators flagged which test cases\n",
    "3. **Analyze metrics** - View aggregated scores and success rates\n",
    "\n",
    "You can also access results programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result and \"run\" in result:\n",
    "    print(f\"\\nüìä Evaluation Details:\")\n",
    "    print(f\"   Run ID: {result['run'].id}\")\n",
    "    print(f\"   Status: {result['run'].status}\")\n",
    "    print(f\"\\nüîó View results in the Agenta UI\")\n",
    "else:\n",
    "    print(\"No result data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data Flow\n",
    "\n",
    "When you run an evaluation, here's what happens:\n",
    "\n",
    "1. **Testcase data flows to the application**\n",
    "   - Input: `{\"country\": \"Germany\", \"capital\": \"Berlin\"}`\n",
    "   - Application receives: `country=\"Germany\"`\n",
    "   - Application returns: `\"Berlin\"`\n",
    "\n",
    "2. **Both testcase data and application output flow to evaluators**\n",
    "   - Evaluator receives: `capital=\"Berlin\"` (from testcase)\n",
    "   - Evaluator receives: `outputs=\"Berlin\"` (from application)\n",
    "   - Evaluator compares and returns: `{\"score\": 1.0, \"success\": True}`\n",
    "\n",
    "3. **Results are stored in Agenta**\n",
    "   - View in web interface\n",
    "   - Access programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've created your first evaluation, explore:\n",
    "\n",
    "- **[Configuring Evaluators](/evaluation/evaluation-from-sdk/configuring-evaluators)** - Create custom scoring logic\n",
    "- **[Managing Testsets](/evaluation/evaluation-from-sdk/managing-testsets)** - Work with test data\n",
    "- **[Running Evaluations](/evaluation/evaluation-from-sdk/running-evaluations)** - Advanced evaluation patterns\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "‚úÖ Define an application with `@ag.application`  \n",
    "‚úÖ Create custom evaluators with `@ag.evaluator`  \n",
    "‚úÖ Use built-in evaluators like LLM-as-a-judge  \n",
    "‚úÖ Create testsets with `ag.testsets.acreate()`  \n",
    "‚úÖ Run evaluations with `aevaluate()`  \n",
    "‚úÖ View results in the Agenta UI  \n",
    "\n",
    "Happy evaluating! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}