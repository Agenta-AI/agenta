{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenta SDK Quick Start - Evaluations\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Create a simple application that returns country capitals\n",
    "2. Create evaluators to check if the application's output is correct\n",
    "3. Run an evaluation to test your application\n",
    "\n",
    "The entire example takes less than 100 lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the Agenta SDK and set up your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Agenta SDK\n",
    "%pip install agenta -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set your API credentials\n",
    "if not os.getenv(\"AGENTA_API_KEY\"):\n",
    "    os.environ[\"AGENTA_API_KEY\"] = getpass(\"Enter your Agenta API key: \")\n",
    "\n",
    "if not os.getenv(\"AGENTA_HOST\"):\n",
    "    os.environ[\"AGENTA_HOST\"] = \"https://cloud.agenta.ai\"  # Change for self-hosted\n",
    "\n",
    "# Set OpenAI API key (required for LLM-as-a-judge evaluator)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"âœ… Environment configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Agenta SDK\n",
    "\n",
    "Initialize the SDK to connect to the Agenta platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12T13:22:23.599Z \u001b[38;5;70m[INFO.]\u001b[0m Agenta -  SDK ver: 0.62.1 \u001b[38;5;245m[agenta.sdk.agenta_init]\u001b[0m \n",
      "2025-11-12T13:22:23.600Z \u001b[38;5;70m[INFO.]\u001b[0m Agenta -  API URL: https://cloud.agenta.ai/api \u001b[38;5;245m[agenta.sdk.agenta_init]\u001b[0m \n",
      "2025-11-12T13:22:23.600Z \u001b[38;5;70m[INFO.]\u001b[0m Agenta - OLTP URL: https://cloud.agenta.ai/api/otlp/v1/traces \u001b[38;5;245m[agenta.sdk.tracing.tracing]\u001b[0m \n",
      "âœ… Agenta SDK initialized!\n"
     ]
    }
   ],
   "source": [
    "import agenta as ag\n",
    "\n",
    "ag.init()\n",
    "\n",
    "print(\"âœ… Agenta SDK initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Your Application\n",
    "\n",
    "An application is any function decorated with `@ag.application`. It receives inputs from test data and returns outputs.\n",
    "\n",
    "Let's create a simple application that returns country capitals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Application defined!\n"
     ]
    }
   ],
   "source": [
    "@ag.application(\n",
    "    slug=\"capital_finder\",\n",
    "    name=\"Capital Finder\",\n",
    "    description=\"Returns the capital of a given country\"\n",
    ")\n",
    "async def capital_finder(country: str):\n",
    "    \"\"\"\n",
    "    A simple application that returns country capitals.\n",
    "    \n",
    "    Args:\n",
    "        country: The country name (from testcase)\n",
    "    \n",
    "    Returns:\n",
    "        The capital city name\n",
    "    \"\"\"\n",
    "    capitals = {\n",
    "        \"Germany\": \"Berlin\",\n",
    "        \"France\": \"Paris\",\n",
    "        \"Spain\": \"Madrid\",\n",
    "        \"Italy\": \"Rome\",\n",
    "    }\n",
    "    return capitals.get(country, \"Unknown\")\n",
    "\n",
    "print(\"âœ… Application defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Custom Evaluators\n",
    "\n",
    "Evaluators check if your application's output is correct. They receive:\n",
    "- Fields from your testcase (e.g., `capital`)\n",
    "- The application's output (always called `outputs`)\n",
    "\n",
    "Let's create two evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluators defined!\n"
     ]
    }
   ],
   "source": [
    "@ag.evaluator(\n",
    "    slug=\"exact_match\",\n",
    "    name=\"Exact Match Evaluator\",\n",
    "    description=\"Checks if the output exactly matches the expected answer\"\n",
    ")\n",
    "async def exact_match(capital: str, outputs: str):\n",
    "    \"\"\"\n",
    "    Evaluates if the application's output matches the expected answer.\n",
    "    \n",
    "    Args:\n",
    "        capital: The expected capital (from testcase)\n",
    "        outputs: What the application returned\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with score and success flag\n",
    "    \"\"\"\n",
    "    is_correct = outputs == capital\n",
    "    return {\n",
    "        \"score\": 1.0 if is_correct else 0.0,\n",
    "        \"success\": is_correct,\n",
    "    }\n",
    "\n",
    "\n",
    "@ag.evaluator(\n",
    "    slug=\"case_insensitive_match\",\n",
    "    name=\"Case Insensitive Match\",\n",
    "    description=\"Checks if output matches ignoring case\"\n",
    ")\n",
    "async def case_insensitive_match(capital: str, outputs: str):\n",
    "    \"\"\"\n",
    "    Evaluates with case-insensitive comparison.\n",
    "    \"\"\"\n",
    "    is_correct = outputs.lower() == capital.lower()\n",
    "    return {\n",
    "        \"score\": 1.0 if is_correct else 0.0,\n",
    "        \"success\": is_correct,\n",
    "    }\n",
    "\n",
    "print(\"âœ… Evaluators defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use Built-in Evaluators\n",
    "\n",
    "Agenta provides built-in evaluators like LLM-as-a-judge. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM judge evaluator created!\n"
     ]
    }
   ],
   "source": [
    "from agenta.sdk.workflows import builtin\n",
    "\n",
    "llm_judge = builtin.auto_ai_critique(\n",
    "    slug=\"llm_judge\",\n",
    "    name=\"LLM Judge Evaluator\",\n",
    "    description=\"Uses an LLM to judge if the answer is correct\",\n",
    "    correct_answer_key=\"capital\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    prompt_template=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a geography expert evaluating answers about world capitals.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Expected capital: {{capital}}\\n\"\n",
    "                \"Student's answer: {{outputs}}\\n\\n\"\n",
    "                \"Is the student's answer correct?\\n\"\n",
    "                \"Respond with ONLY a number from 0.0 (wrong) to 1.0 (correct).\\n\"\n",
    "                \"Nothing else - just the number.\"\n",
    "            ),\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM judge evaluator created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Test Data\n",
    "\n",
    "Define test cases as a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 4 test cases\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\"country\": \"Germany\", \"capital\": \"Berlin\"},\n",
    "    {\"country\": \"France\", \"capital\": \"Paris\"},\n",
    "    {\"country\": \"Spain\", \"capital\": \"Madrid\"},\n",
    "    {\"country\": \"Italy\", \"capital\": \"Rome\"},\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(test_data)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Evaluation\n",
    "\n",
    "Now let's create a testset and run the evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Creating testset...\n",
      "âœ… Testset created with ID: 019a783b-7894-7c80-a5ce-25005d745f5f\n",
      "   Contains 4 test cases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agenta.sdk.evaluations import aevaluate\n",
    "\n",
    "# Create a testset\n",
    "print(\"ğŸ“ Creating testset...\")\n",
    "testset = await ag.testsets.acreate(\n",
    "    name=\"Country Capitals Quick Start\",\n",
    "    data=test_data,\n",
    ")\n",
    "\n",
    "if not testset or not testset.id:\n",
    "    print(\"âŒ Failed to create testset\")\n",
    "else:\n",
    "    print(f\"âœ… Testset created with ID: {testset.id}\")\n",
    "    print(f\"   Contains {len(test_data)} test cases\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Running evaluation...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Evaluation running...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â€¢                   run_id=019a783b-a7dd-7d93-a537-bf8bbd9ea102\n",
      "â”œâ”€ â€¢            testset_id=019a783b-7894-7c80-a5ce-25005d745f5f\n",
      "â”‚  â”‚           -------------------------------------------------------------\n",
      "â”‚  â”œâ”€ â€¢        testcase_id=7aa9d79f-7868-5305-89fc-98fdc4d2f257\n",
      "â”‚  â”‚  â”œâ”€ â€¢     scenario_id=019a783b-a9ca-7881-909c-769d5d982ac6\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-aa39-7901-9268-0fefdf22fc0d (testcase)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-aab6-7d71-ae94-7de9cf001ec6 (invocation)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-ab2a-7de3-b8fa-119c40f9b00e (annotation)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-ab9a-7f83-938d-1b7dc5e908dc (annotation)\n",
      "â”‚  â”‚  â”‚  â””â”€ â€¢    result_id=019a783b-b120-7b42-b846-6a28fc2f610f (annotation)\n",
      "â”‚  â”‚  â””â”€ â€¢      metrics_id=019a783b-b689-7901-b9b6-2be90de568c6\n",
      "â”‚  â”‚           -------------------------------------------------------------\n",
      "â”‚  â”œâ”€ â€¢        testcase_id=a952230e-a234-5c22-8b53-90db8a64adf7\n",
      "â”‚  â”‚  â”œâ”€ â€¢     scenario_id=019a783b-b6cb-7a90-9e64-39518978ce5a\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-b707-7ff0-8c9f-14b463dfa020 (testcase)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-b747-7013-afd0-67f1042420c6 (invocation)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-b7b9-7e22-9b13-9f9193b9be66 (annotation)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-b833-72f3-b571-f21120e65bac (annotation)\n",
      "â”‚  â”‚  â”‚  â””â”€ â€¢    result_id=019a783b-bd01-7653-b790-95bfcb704032 (annotation)\n",
      "â”‚  â”‚  â””â”€ â€¢      metrics_id=019a783b-c24c-7442-8e40-638a6e52f951\n",
      "â”‚  â”‚           -------------------------------------------------------------\n",
      "â”‚  â”œâ”€ â€¢        testcase_id=cd5879dc-d650-5164-8f44-e9dc031ff080\n",
      "â”‚  â”‚  â”œâ”€ â€¢     scenario_id=019a783b-c28d-7ea2-8162-7cc9a61fddcf\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-c2cf-7632-92e8-271356cedcf6 (testcase)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-c30f-7821-b4e6-8a3e4e34bac8 (invocation)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-c380-75d0-b24a-fd68e8ac6e56 (annotation)\n",
      "â”‚  â”‚  â”‚  â”œâ”€ â€¢    result_id=019a783b-c3f2-7191-a643-9ce62704cb09 (annotation)\n",
      "â”‚  â”‚  â”‚  â””â”€ â€¢    result_id=019a783b-c8e2-7dd2-b99f-9e30740117be (annotation)\n",
      "â”‚  â”‚  â””â”€ â€¢      metrics_id=019a783b-ce3b-7cf3-b4c9-6575f6a02fbe\n",
      "â”‚  â”‚           -------------------------------------------------------------\n",
      "â”‚  â””â”€ â€¢        testcase_id=e7c0a205-d422-5dde-b014-66e6e060936c\n",
      "â”‚     â”œâ”€ â€¢     scenario_id=019a783b-ce7b-7e80-b309-3a740e122252\n",
      "â”‚     â”‚  â”œâ”€ â€¢    result_id=019a783b-ceb8-7632-a0f5-c12d5c414462 (testcase)\n",
      "â”‚     â”‚  â”œâ”€ â€¢    result_id=019a783b-cef7-7083-9686-92baf3457d8d (invocation)\n",
      "â”‚     â”‚  â”œâ”€ â€¢    result_id=019a783b-cf6a-78a3-ab30-52e5d98a23da (annotation)\n",
      "â”‚     â”‚  â”œâ”€ â€¢    result_id=019a783b-cfd9-74e0-a5ff-d12a9cd904cb (annotation)\n",
      "â”‚     â”‚  â””â”€ â€¢    result_id=019a783b-d1ad-7a13-a294-c1568a7a1f1f (annotation)\n",
      "â”‚     â””â”€ â€¢      metrics_id=019a783b-d6dd-7463-8fd0-7d1d5f3a02d1\n",
      "â”‚              -------------------------------------------------------------\n",
      "â””â”€ â€¢            metrics_id=019a783b-d7bd-7c30-8ea5-462088b33f96\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Evaluation finished.\n",
      "----------------------------------------------------------------------------\n",
      "Evaluation URL: https://cloud.agenta.ai/w/019a5aa9-7bf0-78d3-a9f6-29dfaddf23d3/p/019a5aa9-7c04-7ad0-aafc-0a1388797bc0/evaluations/results/019a783b-a7dd-7d93-a537-bf8bbd9ea102\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "======================================================================\n",
      "âœ… Evaluation Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with all three evaluators\n",
    "print(\"ğŸš€ Running evaluation...\\n\")\n",
    "\n",
    "result = await aevaluate(\n",
    "    name=\"My First Eval\",\n",
    "    description=\"Test cases for capital city questions\",\n",
    "    testsets=[testset.id],\n",
    "    applications=[capital_finder],\n",
    "    evaluators=[\n",
    "        exact_match,\n",
    "        case_insensitive_match,\n",
    "        llm_judge,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Evaluation Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "The evaluation results are now available in the Agenta UI! You can:\n",
    "\n",
    "1. **View detailed results** - See how each test case performed\n",
    "2. **Compare evaluators** - See which evaluators flagged which test cases\n",
    "3. **Analyze metrics** - View aggregated scores and success rates\n",
    "\n",
    "You can also access results programmatically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data Flow\n",
    "\n",
    "When you run an evaluation, here's what happens:\n",
    "\n",
    "1. **Testcase data flows to the application**\n",
    "   - Input: `{\"country\": \"Germany\", \"capital\": \"Berlin\"}`\n",
    "   - Application receives: `country=\"Germany\"`\n",
    "   - Application returns: `\"Berlin\"`\n",
    "\n",
    "2. **Both testcase data and application output flow to evaluators**\n",
    "   - Evaluator receives: `capital=\"Berlin\"` (from testcase)\n",
    "   - Evaluator receives: `outputs=\"Berlin\"` (from application)\n",
    "   - Evaluator compares and returns: `{\"score\": 1.0, \"success\": True}`\n",
    "\n",
    "3. **Results are stored in Agenta**\n",
    "   - View in web interface\n",
    "   - Access programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've created your first evaluation, explore:\n",
    "\n",
    "- **[Configuring Evaluators](https://docs.agenta.ai/evaluation/evaluation-from-sdk/configuring-evaluators)** - Create custom scoring logic\n",
    "- **[Managing Testsets](https://docs.agenta.ai/evaluation/evaluation-from-sdk/managing-testsets)** - Work with test data\n",
    "- **[Running Evaluations](https://docs.agenta.ai/evaluation/evaluation-from-sdk/running-evaluations)** - Advanced evaluation patterns\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "âœ… Define an application with `@ag.application`  \n",
    "âœ… Create custom evaluators with `@ag.evaluator`  \n",
    "âœ… Use built-in evaluators like LLM-as-a-judge  \n",
    "âœ… Create testsets with `ag.testsets.acreate()`  \n",
    "âœ… Run evaluations with `aevaluate()`  \n",
    "âœ… View results in the Agenta UI  \n",
    "\n",
    "Happy evaluating! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
